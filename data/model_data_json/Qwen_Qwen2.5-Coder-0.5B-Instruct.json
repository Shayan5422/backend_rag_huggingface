{
    "model_id": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
    "downloads": 114663,
    "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "code",
        "codeqwen",
        "chat",
        "qwen",
        "qwen-coder",
        "conversational",
        "en",
        "arxiv:2409.12186",
        "arxiv:2407.10671",
        "base_model:Qwen/Qwen2.5-Coder-0.5B",
        "base_model:finetune:Qwen/Qwen2.5-Coder-0.5B",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: apache-2.0 license_link: language: - en base_model: - Qwen/Qwen2.5-Coder-0.5B pipeline_tag: text-generation library_name: transformers tags: - code - codeqwen - chat - qwen - qwen-coder --- # Qwen2.5-Coder-0.5B-Instruct ## Introduction Qwen2.5-Coder is the latest series of Code-Specific Qwen large language models (formerly known as CodeQwen). As of now, Qwen2.5-Coder has covered six mainstream model sizes, 0.5, 1.5, 3, 7, 14, 32 billion parameters, to meet the needs of different developers. Qwen2.5-Coder brings the following improvements upon CodeQwen1.5: - Significantly improvements in **code generation**, **code reasoning** and **code fixing**. Base on the strong Qwen2.5, we scale up the training tokens into 5.5 trillion including source code, text-code grounding, Synthetic data, etc. Qwen2.5-Coder-32B has become the current state-of-the-art open-source codeLLM, with its coding abilities matching those of GPT-4o. - A more comprehensive foundation for real-world applications such as **Code Agents**. Not only enhancing coding capabilities but also maintaining its strengths in mathematics and general competencies. **This repo contains the instruction-tuned 0.5B Qwen2.5-Coder model**, which has the following features: - Type: Causal Language Models - Training Stage: Pretraining & Post-training - Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings - Number of Parameters: 0.49B - Number of Paramaters (Non-Embedding): 0.36B - Number of Layers: 24 - Number of Attention Heads (GQA): 14 for Q and 2 for KV - Context Length: Full 32,768 tokens For more details, please refer to our blog, GitHub, Documentation, Arxiv. ## Requirements The code of Qwen2.5-Coder has been in the latest Hugging face and we advise you to use the latest version of . With , you will encounter the following error: ## Quickstart Here provides a code snippet with to show you how to load the tokenizer and model and how to generate contents. ## Evaluation & Performance Detailed evaluation results are reported in this ðŸ“‘ blog. For requirements on GPU memory and the respective throughput, see results here. ## Citation If you find our work helpful, feel free to give us a cite.",
    "model_explanation_gemini": "A 0.5-billion-parameter instruction-tuned language model specialized in code generation, reasoning, and fixing, designed for real-world coding applications."
}