{
    "model_id": "sentence-transformers/paraphrase-albert-small-v2",
    "downloads": 177093,
    "tags": [
        "sentence-transformers",
        "pytorch",
        "tf",
        "rust",
        "onnx",
        "safetensors",
        "openvino",
        "albert",
        "feature-extraction",
        "sentence-similarity",
        "transformers",
        "dataset:flax-sentence-embeddings/stackexchange_xml",
        "dataset:s2orc",
        "dataset:ms_marco",
        "dataset:wiki_atomic_edits",
        "dataset:snli",
        "dataset:multi_nli",
        "dataset:embedding-data/altlex",
        "dataset:embedding-data/simple-wiki",
        "dataset:embedding-data/flickr30k-captions",
        "dataset:embedding-data/coco_captions",
        "dataset:embedding-data/sentence-compression",
        "dataset:embedding-data/QQP",
        "dataset:yahoo_answers_topics",
        "arxiv:1908.10084",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: apache-2.0 library_name: sentence-transformers tags: - sentence-transformers - feature-extraction - sentence-similarity - transformers datasets: - flax-sentence-embeddings/stackexchange_xml - s2orc - ms_marco - wiki_atomic_edits - snli - multi_nli - embedding-data/altlex - embedding-data/simple-wiki - embedding-data/flickr30k-captions - embedding-data/coco_captions - embedding-data/sentence-compression - embedding-data/QQP - yahoo_answers_topics pipeline_tag: sentence-similarity --- # sentence-transformers/paraphrase-albert-small-v2 This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search. ## Usage (Sentence-Transformers) Using this model becomes easy when you have sentence-transformers installed: Then you can use the model like this: ## Usage (HuggingFace Transformers) Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings. ## Full Model Architecture ## Citing & Authors This model was trained by sentence-transformers. If you find this model helpful, feel free to cite our publication Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks:",
    "model_explanation_gemini": "Maps sentences and paragraphs to 768-dimensional vectors for tasks like clustering and semantic search."
}