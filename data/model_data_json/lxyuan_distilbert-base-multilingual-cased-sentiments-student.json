{
    "model_id": "lxyuan/distilbert-base-multilingual-cased-sentiments-student",
    "downloads": 478991,
    "tags": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "distilbert",
        "text-classification",
        "sentiment-analysis",
        "zero-shot-distillation",
        "distillation",
        "zero-shot-classification",
        "debarta-v3",
        "en",
        "ar",
        "de",
        "es",
        "fr",
        "ja",
        "zh",
        "id",
        "hi",
        "it",
        "ms",
        "pt",
        "dataset:tyqiangz/multilingual-sentiments",
        "doi:10.57967/hf/1422",
        "license:apache-2.0",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: apache-2.0 tags: - sentiment-analysis - text-classification - zero-shot-distillation - distillation - zero-shot-classification - debarta-v3 model-index: - name: distilbert-base-multilingual-cased-sentiments-student results: [] datasets: - tyqiangz/multilingual-sentiments language: - en - ar - de - es - fr - ja - zh - id - hi - it - ms - pt --- <!-- This model card has been generated automatically according to the information the Trainer had access to. You should probably proofread and complete it, then remove this comment. --> # distilbert-base-multilingual-cased-sentiments-student This model is distilled from the zero-shot classification pipeline on the Multilingual Sentiment dataset using this script. In reality the multilingual-sentiment dataset is annotated of course, but we'll pretend and ignore the annotations for the sake of example. Teacher model: MoritzLaurer/mDeBERTa-v3-base-mnli-xnli Teacher hypothesis template: \"The sentiment of this text is {}.\" Student model: distilbert-base-multilingual-cased ## Inference example ## Training procedure Notebook link: here ### Training hyperparameters Result can be reproduce using the following commands: If you are training this model on Colab, make the following code changes to avoid Out-of-memory error message: ### Training log ### Framework versions - Transformers 4.28.1 - Pytorch 2.0.0+cu118 - Datasets 2.11.0 - Tokenizers 0.13.3",
    "model_explanation_gemini": "A distilled multilingual sentiment analysis model trained to classify text sentiment across multiple languages using zero-shot distillation from a teacher model."
}