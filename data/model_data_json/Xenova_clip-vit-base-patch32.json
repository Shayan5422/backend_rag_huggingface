{
    "model_id": "Xenova/clip-vit-base-patch32",
    "downloads": 139162,
    "tags": [
        "transformers.js",
        "onnx",
        "clip",
        "zero-shot-image-classification",
        "base_model:openai/clip-vit-base-patch32",
        "base_model:quantized:openai/clip-vit-base-patch32",
        "region:us"
    ],
    "description": "--- base_model: openai/clip-vit-base-patch32 library_name: transformers.js --- with ONNX weights to be compatible with Transformers.js. ## Usage (Transformers.js) If you haven't already, you can install the Transformers.js JavaScript library from NPM using: **Example:** Perform zero-shot image classification with the API. --- Note: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ðŸ¤— Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named ).",
    "model_explanation_gemini": "Enables zero-shot image classification in web environments using ONNX-optimized CLIP ViT-B/32 model via Transformers.js."
}