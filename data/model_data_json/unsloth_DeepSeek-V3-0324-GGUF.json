{
    "model_id": "unsloth/DeepSeek-V3-0324-GGUF",
    "downloads": 78841,
    "tags": [
        "transformers",
        "gguf",
        "deepseek_v3",
        "text-generation",
        "deepseek",
        "unsloth",
        "custom_code",
        "en",
        "arxiv:2412.19437",
        "base_model:deepseek-ai/DeepSeek-V3-0324",
        "base_model:quantized:deepseek-ai/DeepSeek-V3-0324",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "fp8",
        "region:us",
        "conversational"
    ],
    "description": "--- base_model: deepseek-ai/DeepSeek-V3-0324 language: - en library_name: transformers license: mit tags: - deepseek_v3 - deepseek - unsloth - transformers --- <div> <p style=\"margin-bottom: 0; margin-top: 0;\"> <strong>Read <a href=\" guide</a> for detailed instructions on running DeepSeek-V3-0324 locally.</strong> </p> <p style=\"margin-bottom: 0;\"> <em>Unsloth's <a href=\" Quants</a> is selectively quantized, greatly improving accuracy over standard bits.</em> </p> <div style=\"display: flex; gap: 5px; align-items: center; \"> <a href=\" <img src=\" width=\"133\"> </a> <a href=\" <img src=\" width=\"173\"> </a> <a href=\" <img src=\" width=\"143\"> </a> </div> <h1 style=\"margin-top: 0rem;\">DeepSeek-V3-0324 Dynamic GGUF</h2> </div> Our DeepSeek-V3-0324 GGUFs allow you to run the model in llama.cpp, LMStudio, Open WebUI and other inference frameworks. Includes 1-4-bit Dynamic versions, which yields better accuracy and results than standard quantization. | MoE Bits | Type | Disk Size | Accuracy | Link | Details | |----------|----------|-------------|----------|------------------------------------------------------------------------------------------------------------|---------------------------------------------------| | 1.78bit (prelim) | IQ1_S | **186GB** | Ok | Link | in MoE mixture of 2.06/1.78bit | | 1.93bit (prelim) | IQ1_M | **196GB** | Fair | Link | in MoE mixture of 2.06/1.93bit | | 2.42bit | IQ2_XXS | **219GB** | Recommended | Link | in MoE all 2.42bit | | 2.71bit | Q2_K_XL | **248GB** | Recommended | Link | in MoE mixture of 3.5/2.71bit | | 3.5bit | Q3_K_XL | **321GB** | Great | Link | in MoE mixture of 4.5/3.5bit | | 4.5bit | Q4_K_XL | **405GB** | Best | Link | in MoE mixture of 5.5/4.5bit | Prelim = preliminary - through our testing, they're generally fine but sometimes don't produce the best code and so more work/testing needs to be done. 2.71bit was found to be the best in terms of performance/size and produces code that is great and works well. 2.42bit was also found to pass all our tests. So, for best results, use the 2.42-bit (IQ2_XXS) or 2.71-bit (Q2_K_XL) versions. Though not a must, try to have at least 180GB+ combined VRAM + RAM. Thank you to the DeepSeek team for releasing their March update to the DeepSeek V3 models. Also thank you to bartowski for providing imatric V3 quants. # Finetune your own Reasoning model like R1 with Unsloth! We have a free Google Colab notebook for turning Llama 3.1 (8B) into a reasoning model: ## ✨ Finetune for Free All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face. | Unsloth supports | Free Notebooks | Performance | Memory use | |-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------| | **GRPO with Phi-4 (14B)** | ▶️ Start on Colab-GRPO.ipynb) | 2x faster | 80% less | | **Llama-3.2 (3B)** | ▶️ Start on Colab-Conversational.ipynb) | 2.4x faster | 58% less | | **Llama-3.2 (11B vision)** | ▶️ Start on Colab-Vision.ipynb) | 2x faster | 60% less | | **Qwen2 VL (7B)** | ▶️ Start on Colab-Vision.ipynb) | 1.8x faster | 60% less | | **Qwen2.5 (7B)** | ▶️ Start on Colab-Alpaca.ipynb) | 2x faster | 60% less | | **Llama-3.1 (8B)** | ▶️ Start on Colab-Alpaca.ipynb) | 2.4x faster | 58% less | | **Phi-3.5 (mini)** | ▶️ Start on Colab | 2x faster | 50% less | | **Gemma 2 (9B)** | ▶️ Start on Colab-Alpaca.ipynb) | 2.4x faster | 58% less | | **Mistral (7B)** | ▶️ Start on Colab-Conversational.ipynb) | 2.2x faster | 62% less | <div align=\"center\"> <img src=\" width=\"60%\" alt=\"DeepSeek-V3\" /> </div> <hr> <div align=\"center\" style=\"line-height: 1;\"> <a href=\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Homepage\" src=\" style=\"display: inline-block; vertical-align: middle;\"/> </a> <a href=\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\" style=\"display: inline-block; vertical-align: middle;\"/> </a> <a href=\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Hugging Face\" src=\" style=\"display: inline-block; vertical-align: middle;\"/> </a> </div> <div align=\"center\" style=\"line-height: 1;\"> <a href=\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Discord\" src=\" style=\"display: inline-block; vertical-align: middle;\"/> </a> <a href=\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Wechat\" src=\" style=\"display: inline-block; vertical-align: middle;\"/> </a> <a href=\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Twitter Follow\" src=\" style=\"display: inline-block; vertical-align: middle;\"/> </a> </div> <div align=\"center\" style=\"line-height: 1;\"> <a href=\"LICENSE\" style=\"margin: 2px;\"> <img alt=\"License\" src=\" style=\"display: inline-block; vertical-align: middle;\"/> </a> </div> ## Features DeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects. !Model Performance ### Reasoning Capabilities - Significant improvements in benchmark performance: - MMLU-Pro: 75.9 → 81.2 (+5.3) - GPQA: 59.1 → 68.4 (+9.3) - AIME: 39.6 → 59.4 (+19.8) - LiveCodeBench: 39.2 → 49.2 (+10.0) ### Front-End Web Development - Improved the executability of the code - More aesthetically pleasing web pages and game front-ends ### Chinese Writing Proficiency - Enhanced style and content quality: - Aligned with the R1 writing style - Better quality in medium-to-long-form writing - Feature Enhancements - Improved multi-turn interactive rewriting - Optimized translation quality and letter writing ### Chinese Search Capabilities - Enhanced report analysis requests with more detailed outputs ### Function Calling Improvements - Increased accuracy in Function Calling, fixing issues from previous V3 versions --- ## Usage Recommendations ### System Prompt In the official DeepSeek web/app, we use the same system prompt with a specific date. For example, ### Temperature In our web and application environments, the temperature parameter $T_{model}$ is set to 0.3. Because many users use the default temperature 1.0 in API call, we have implemented an API temperature $T_{api}$ mapping mechanism that adjusts the input API temperature value of 1.0 to the most suitable model temperature setting of 0.3. $$ T_{model} = T_{api} \\times 0.3 \\quad (0 \\leq T_{api} \\leq 1) $$ $$ T_{model} = T_{api} - 0.7 \\quad (1 < T_{api} \\leq 2) $$ Thus, if you call V3 via API, temperature 1.0 equals to the model temperature 0.3. ### Prompts for File Uploading and Web Search For file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments. For Web Search, {search_results}, {cur_date}, and {question} are arguments. For Chinese query, we use the prompt: For English query, we use the prompt: ## How to Run Locally The model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3. Please visit DeepSeek-V3 repo for more information about running this model locally. **This model supports features such as function calling, JSON output, and FIM completion. For instructions on how to construct prompts to use these features, please refer to DeepSeek-V2.5 repo.** **NOTE: Hugging Face's Transformers has not been directly supported yet.** ## License This repository and the model weights are licensed under the MIT License. ## Citation ## Contact If you have any questions, please raise an issue or contact us at service@deepseek.com."
}