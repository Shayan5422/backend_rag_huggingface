{
    "model_id": "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "downloads": 209237,
    "tags": [
        "transformers",
        "safetensors",
        "mistral",
        "text-generation",
        "unsloth",
        "mistral-7b",
        "mistral-instruct",
        "instruct",
        "conversational",
        "en",
        "base_model:mistralai/Mistral-7B-Instruct-v0.3",
        "base_model:quantized:mistralai/Mistral-7B-Instruct-v0.3",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "4-bit",
        "bitsandbytes",
        "region:us"
    ],
    "description": "--- language: - en library_name: transformers license: apache-2.0 tags: - unsloth - transformers - mistral - mistral-7b - mistral-instruct - instruct base_model: mistralai/Mistral-7B-Instruct-v0.3 --- # Finetune Mistral, Gemma, Llama 2-5x faster with 70% less memory via Unsloth! We have a Google Colab Tesla T4 notebook for Mistral v3 7b here: For conversational ShareGPT style and using Mistral v3 Instruct: <img src=\" width=\"200\"/> <img src=\" width=\"200\"/> ## ✨ Finetune for Free All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face. | Unsloth supports | Free Notebooks | Performance | Memory use | |-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------| | **Llama-3.2 (3B)** | ▶️ Start on Colab | 2.4x faster | 58% less | | **Llama-3.2 (11B vision)** | ▶️ Start on Colab | 2x faster | 60% less | | **Llama-3.1 (8B)** | ▶️ Start on Colab | 2.4x faster | 58% less | | **Qwen2 VL (7B)** | ▶️ Start on Colab | 1.8x faster | 60% less | | **Qwen2.5 (7B)** | ▶️ Start on Colab | 2x faster | 60% less | | **Phi-3.5 (mini)** | ▶️ Start on Colab | 2x faster | 50% less | | **Gemma 2 (9B)** | ▶️ Start on Colab | 2.4x faster | 58% less | | **Mistral (7B)** | ▶️ Start on Colab | 2.2x faster | 62% less | | **DPO - Zephyr** | ▶️ Start on Colab | 1.9x faster | 19% less | <img src=\" width=\"200\"/> - This conversational notebook is useful for ShareGPT ChatML / Vicuna templates. - This text completion notebook is for raw text. This DPO notebook replicates Zephyr. - \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster.",
    "model_explanation_gemini": "An optimized 4-bit quantized version of Mistral-7B-Instruct-v0.3 for faster and more memory-efficient fine-tuning of conversational AI models."
}