{
    "model_id": "naver-clova-ix/donut-base-finetuned-docvqa",
    "downloads": 143391,
    "tags": [
        "transformers",
        "pytorch",
        "vision-encoder-decoder",
        "image-text-to-text",
        "donut",
        "image-to-text",
        "vision",
        "document-question-answering",
        "arxiv:2111.15664",
        "license:mit",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: mit pipeline_tag: document-question-answering tags: - donut - image-to-text - vision widget: - text: \"What is the invoice number?\" src: \" - text: \"What is the purchase amount?\" src: \" --- # Donut (base-sized model, fine-tuned on DocVQA) Donut model fine-tuned on DocVQA. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Disclaimer: The team releasing Donut did not write a model card for this model so this model card has been written by the Hugging Face team. ## Model description Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. !model image ## Intended uses & limitations This model is fine-tuned on DocVQA, a document visual question answering dataset. We refer to the documentation which includes code examples. ### BibTeX entry and citation info",
    "model_explanation_gemini": "Performs document visual question answering by generating text answers from images of documents using a vision encoder and text decoder."
}