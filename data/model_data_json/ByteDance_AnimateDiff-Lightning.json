{
    "model_id": "ByteDance/AnimateDiff-Lightning",
    "downloads": 132463,
    "tags": [
        "diffusers",
        "text-to-video",
        "stable-diffusion",
        "animatediff",
        "arxiv:2403.12706",
        "license:creativeml-openrail-m",
        "region:us"
    ],
    "description": "--- license: creativeml-openrail-m tags: - text-to-video - stable-diffusion - animatediff library_name: diffusers inference: false --- # AnimateDiff-Lightning <video src=' width=\"100%\" autoplay muted loop playsinline style='margin:0'></video> <video src=' width=\"100%\" autoplay muted loop playsinline style='margin:0'></video> AnimateDiff-Lightning is a lightning-fast text-to-video generation model. It can generate videos more than ten times faster than the original AnimateDiff. For more information, please refer to our research paper: AnimateDiff-Lightning: Cross-Model Diffusion Distillation. We release the model as part of the research. Our models are distilled from AnimateDiff SD1.5 v2. This repository contains checkpoints for 1-step, 2-step, 4-step, and 8-step distilled models. The generation quality of our 2-step, 4-step, and 8-step model is great. Our 1-step model is only provided for research purposes. ## Demo Try AnimateDiff-Lightning using our text-to-video generation demo. ## Recommendation AnimateDiff-Lightning produces the best results when used with stylized base models. We recommend using the following base models: Realistic - epiCRealism - Realistic Vision - DreamShaper - AbsoluteReality - MajicMix Realistic Anime & Cartoon - ToonYou - IMP - Mistoon Anime - DynaVision - RCNZ Cartoon 3d - MajicMix Reverie Additionally, feel free to explore different settings. We find using 3 inference steps on the 2-step model produces great results. We find certain base models produces better results with CFG. We also recommend using Motion LoRAs as they produce stronger motion. We use Motion LoRAs with strength 0.7~0.8 to avoid watermark. ## Diffusers Usage ## ComfyUI Usage 1. Download animatediff_lightning_workflow.json and import it in ComfyUI. 1. Install nodes. You can install them manually or use ComfyUI-Manager. * ComfyUI-AnimateDiff-Evolved * ComfyUI-VideoHelperSuite 1. Download your favorite base model checkpoint and put them under 1. Download AnimateDiff-Lightning checkpoint and put them under !ComfyUI Workflow ## Video-to-Video Generation AnimateDiff-Lightning is great for video-to-video generation. We provide the simplist comfyui workflow using ControlNet. 1. Download animatediff_lightning_v2v_openpose_workflow.json and import it in ComfyUI. 1. Install nodes. You can install them manually or use ComfyUI-Manager. * ComfyUI-AnimateDiff-Evolved * ComfyUI-VideoHelperSuite * ComfyUI-Advanced-ControlNet * comfyui_controlnet_aux 1. Download your favorite base model checkpoint and put them under 1. Download AnimateDiff-Lightning checkpoint and put them under 1. Download ControlNet OpenPose checkpoint to 1. Upload your video and run the pipeline. Additional notes: 1. Video shouldn't be too long or too high resolution. We used 576x1024 8 second 30fps videos for testing. 1. Set the frame rate to match your input video. This allows audio to match with the output video. 1. DWPose will download checkpoint itself on its first run. 1. DWPose may get stuck in UI, but the pipeline is actually still running in the background. Check ComfyUI log and your output folder. !ComfyUI OpenPose Workflow # Cite Our Work",
    "model_explanation_gemini": "Generates videos from text inputs significantly faster than the original AnimateDiff model, optimized for speed and quality with distilled checkpoints for various step configurations."
}