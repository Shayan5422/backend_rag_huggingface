{
    "model_id": "Qwen/Qwen2.5-Math-1.5B",
    "downloads": 158341,
    "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "conversational",
        "en",
        "arxiv:2409.12122",
        "base_model:Qwen/Qwen2.5-1.5B",
        "base_model:finetune:Qwen/Qwen2.5-1.5B",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- base_model: Qwen/Qwen2.5-1.5B language: - en pipeline_tag: text-generation library_name: transformers license: apache-2.0 license_link: --- # Qwen2.5-Math-1.5B > [!Warning] > <div align=\"center\"> > <b> > ðŸš¨ Qwen2.5-Math mainly supports solving English and Chinese math problems through CoT and TIR. We do not recommend using this series of models for other tasks. > </b> > </div> ## Introduction In August 2024, we released the first series of mathematical LLMs - Qwen2-Math - of our Qwen family. A month later, we have upgraded it and open-sourced **Qwen2.5-Math** series, including base models **Qwen2.5-Math-1.5B/7B/72B**, instruction-tuned models **Qwen2.5-Math-1.5B/7B/72B-Instruct**, and mathematical reward model **Qwen2.5-Math-RM-72B**. Unlike Qwen2-Math series which only supports using Chain-of-Thught (CoT) to solve English math problems, Qwen2.5-Math series is expanded to support using both CoT and Tool-integrated Reasoning (TIR) to solve math problems in both Chinese and English. The Qwen2.5-Math series models have achieved significant performance improvements compared to the Qwen2-Math series models on the Chinese and English mathematics benchmarks with CoT. ![]( While CoT plays a vital role in enhancing the reasoning capabilities of LLMs, it faces challenges in achieving computational accuracy and handling complex mathematical or algorithmic reasoning tasks, such as finding the roots of a quadratic equation or computing the eigenvalues of a matrix. TIR can further improve the model's proficiency in precise computation, symbolic manipulation, and algorithmic manipulation. Qwen2.5-Math-1.5B/7B/72B-Instruct achieve 79.7, 85.3, and 87.8 respectively on the MATH benchmark using TIR. ## Model Details For more details, please refer to our blog post and GitHub repo. ## Requirements * for Qwen2.5-Math models. The latest version is recommended. > [!Warning] > <div align=\"center\"> > <b> > ðŸš¨ This is a must because <code>transformers</code> integrated Qwen2 codes since <code>4.37.0</code>. > </b> > </div> For requirements on GPU memory and the respective throughput, see similar results of Qwen2 here. ## Quick Start > [!Important] > > **Qwen2.5-Math-1.5B-Instruct** is an instruction model for chatting; > > **Qwen2.5-Math-1.5B** is a base model typically used for completion and few-shot inference, serving as a better starting point for fine-tuning. ## Citation If you find our work helpful, feel free to give us a citation.",
    "model_explanation_gemini": "Specialized in solving English and Chinese math problems using Chain-of-Thought (CoT) and Tool-integrated Reasoning (TIR) techniques."
}