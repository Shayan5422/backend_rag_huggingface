{
    "model_id": "genbio-ai/AIDO.Protein-RAG-3B",
    "downloads": 134247,
    "tags": [
        "pytorch",
        "fm4bio",
        "arxiv:2412.06993",
        "arxiv:2406.05347",
        "license:other",
        "region:us"
    ],
    "description": "--- license: other --- # AIDO.Protein-RAG-3B AIDO.Protein-RAG-3B (AIDO.RAGPLM) is a pretrained Retrieval-Augmented protein language model within an AI-driven Digital Organism framework. This model, along with AIDO.RAGFold, integrates pretrained protein language models with retrieved Multiple Sequence Alignments (MSA), enabling the incorporation of co-evolutionary information for structure prediction while compensating for limited MSA data through large-scale pretraining. AIDO.Protein-RAG-3B outperforms single-sequence protein language models in perplexity, contact prediction, and fitness prediction. When used as a feature extractor for structure prediction in AIDO.RAGFold, it achieves TM-scores comparable to AlphaFold2 with sufficient MSA data (8x faster runtime), and significantly surpasses AlphaFold2 in MSA-limited scenarios (∆TM-score=0.379, 0.116, and 0.059 for 0, 5, and 10 input sequences respectively). ## Model Architecture AIDO.Protein-RAG-3B employs a transformer encoder-only architecture with dense MLP layers in each block (Panel **​c**​ below). The model uses single amino acid tokenization and is optimized via masked language modeling (MLM). <center><img src=\"architecture.png\" alt=\"An Overview of AIDO.Protein\" style=\"width:90%; height:auto;\" /></center> More architecture details are shown below: | Model Arch | Value | | ------------------ | :---: | | Num Attention Head | 40 | | Num Hidden Layer | 36 | | Hidden Size | 2560 | | FFN Hidden Size | 6832 | | Context Length | 12.8K | ## Pre-training ### Data Preparation **UniRef50/Uniclust30 MSA dataset**: We utilized sequences from UniRef50 as queries to search for homologous sequences in UniClust30, subsequently constructing multiple sequence alignments (MSAs). UniRef50 comprises a total of 53.6 million sequences. Using HHblits, we searched all sequences, identifying over 25 homologous sequences for 23.7 million of them. This dataset was directly used as the training set, referred to as . The remaining 29.9 million sequences were input into MSA Retriever, resulting in 7.7 million sequences with more than 25 homologous sequences. This dataset was designated as . During training, RAGPLM randomly sampled from the two datasets with probabilities of 0.75 and 0.25 ### Training Details We fine-tuned a pretrained masked language model with 3-billion parameters (MLM-3B) using MSA data by concatenating the query sequence with homologous sequences. We introduced several modifications to the standard BERT masking strategy: (1) We randomly sampled span positions from a query sequence of length , with span lengths following a geometric distribution (), and capped the maximum length at 10. Our experiments revealed that this settings lead to an average of 15% of the query tokens were masked. (2) To prevent information leakage, when a residue was selected, all residues at the same index across all sequences (the column of the MSA matrix) were also masked. (3) When a column of MSA was selected for masking, the entire column was replaced with the token in 80% of cases, with random amino acids in 10% of cases, and remained unchanged in the remaining 10% of cases. To help the model distinguish which tokens are from the same chain and which tokens have the same residue index, we use 2D rotary position embedding to encode the tokens. | | MLM-3B | AIDO.Protein-RAG-3B | | ---------------- | ------------------ | -------------------------- | | Training data | UniRef+ColabFoldDB | HHblits_MSA, Retriever_MSA | | Initial params | Random | MLM-3B | | Learning rate | 2.5e-4 | 1e-4 | | Training tokens | 1000B | 100B | | Batch size | 2560 | 256 | | Micro batch size | 4 | 1 | | Sample length | 1024 | 12,800 | | Attention | Bi-directional | Bi-directional | ### Tokenization We encode protein sequence with single amino acid resolution with 44 vocabularies, where 24 tokens represent amino acid types and 20 are special tokens. Sequences were also suffixed with a token as hooks for downstream tasks. ## Evaluation of AIDO.Protein-RAG-3B AIDO.Protein-RAG-3B surpasses single-sequence protein language models in perplexity, contact prediction, and fitness prediction. Subsequently, we utilized AIDO.Protein-RAG-3B as a feature extractor, integrating it with the folding trunks and Structure Modules to achieve end-to-end structural prediction (AIDO.RAGFold). Our findings indicate that when sufficient MSA is available, our method achieves results comparable to AlphaFold2 and is eight times faster; when MSA is insufficient, our method significantly outperforms AlphaFold2. ## Results ### Unsupervised Contact Prediction <center><img src=\"unsupervised_contact_prediction.png\" alt=\"unsupervised_contact_prediction\" style=\"width:90%; height:auto;\" /></center> ### Supervised downstream tasks <center><img src=\"supervised_tasks.png\" alt=\"supervised_tasks\" style=\"width:90%; height:auto;\" /></center> ### AIDO.RAGFold <center><img src=\"structure_prediction.png\" alt=\"structure_prediction\" style=\"width:90%; height:auto;\" /></center> ## How to Use ### Build Downstream Models Using ModelGenerator For more information, visit: Model Generator ### Use Directly in Python #### Embedding #### Sequence Level Classification #### Token Level Classification #### Sequence Level Regression # Citation Please cite AIDO.Protein-RAG-3B using the following BibTex code:",
    "model_explanation_gemini": "AIDO.Protein-RAG-3B is a retrieval-augmented protein language model that integrates multiple sequence alignments for improved protein structure prediction, especially in MSA-limited scenarios, while outperforming single-sequence models in perplexity, contact prediction, and fitness prediction."
}