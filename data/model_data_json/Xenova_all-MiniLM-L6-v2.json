{
    "model_id": "Xenova/all-MiniLM-L6-v2",
    "downloads": 334341,
    "tags": [
        "transformers.js",
        "onnx",
        "bert",
        "feature-extraction",
        "base_model:sentence-transformers/all-MiniLM-L6-v2",
        "base_model:quantized:sentence-transformers/all-MiniLM-L6-v2",
        "license:apache-2.0",
        "region:us"
    ],
    "description": "--- base_model: sentence-transformers/all-MiniLM-L6-v2 library_name: transformers.js license: apache-2.0 --- with ONNX weights to be compatible with Transformers.js. ## Usage (Transformers.js) If you haven't already, you can install the Transformers.js JavaScript library from NPM using: You can then use the model to compute embeddings like this: You can convert this Tensor to a nested JavaScript array using : Note: Having a separate repo for ONNX weights is intended to be a temporary solution until WebML gains more traction. If you would like to make your models web-ready, we recommend converting to ONNX using ðŸ¤— Optimum and structuring your repo like this one (with ONNX weights located in a subfolder named ).",
    "model_explanation_gemini": "Converts text into embeddings for web applications using ONNX weights compatible with Transformers.js."
}