{
    "model_id": "cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-multi-all",
    "downloads": 36833,
    "tags": [
        "transformers",
        "pytorch",
        "roberta",
        "text-classification",
        "dataset:cardiffnlp/tweet_topic_multi",
        "model-index",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- datasets: - cardiffnlp/tweet_topic_multi metrics: - f1 - accuracy model-index: - name: cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-multi-all results: - task: type: text-classification name: Text Classification dataset: name: cardiffnlp/tweet_topic_multi type: cardiffnlp/tweet_topic_multi args: cardiffnlp/tweet_topic_multi split: test_2021 metrics: - name: F1 type: f1 value: 0.7647668393782383 - name: F1 (macro) type: f1_macro value: 0.6187022581213811 - name: Accuracy type: accuracy value: 0.5485407980941036 pipeline_tag: text-classification widget: - text: \"I'm sure the {@Tampa Bay Lightning@} wouldâ€™ve rather faced the Flyers but man does their experience versus the Blue Jackets this year and last help them a lot versus this Islanders team. Another meat grinder upcoming for the good guys\" example_title: \"Example 1\" - text: \"Love to take night time bike rides at the jersey shore. Seaside Heights boardwalk. Beautiful weather. Wishing everyone a safe Labor Day weekend in the US.\" example_title: \"Example 2\" --- # cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-multi-all This model is a fine-tuned version of cardiffnlp/twitter-roberta-base-dec2021 on the tweet_topic_multi. This model is fine-tuned on split and validated on split of tweet_topic. Fine-tuning script can be found here. It achieves the following results on the test_2021 set: - F1 (micro): 0.7647668393782383 - F1 (macro): 0.6187022581213811 - Accuracy: 0.5485407980941036 ### Usage ### Reference",
    "model_explanation_gemini": "Classifies tweets into multiple topics using a fine-tuned RoBERTa model trained on Twitter data."
}