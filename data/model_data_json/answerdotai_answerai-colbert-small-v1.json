{
    "model_id": "answerdotai/answerai-colbert-small-v1",
    "downloads": 2356105,
    "tags": [
        "onnx",
        "safetensors",
        "bert",
        "ColBERT",
        "RAGatouille",
        "passage-retrieval",
        "en",
        "arxiv:2407.20750",
        "license:apache-2.0",
        "region:us"
    ],
    "description": "--- license: apache-2.0 language: - en tags: - ColBERT - RAGatouille - passage-retrieval --- # answerai-colbert-small-v1 **answerai-colbert-small-v1** is a new, proof-of-concept model by Answer.AI, showing the strong performance multi-vector models with the new JaColBERTv2.5 training recipe and some extra tweaks can reach, even with just **33 million parameters**. While being MiniLM-sized, it outperforms all previous similarly-sized models on common benchmarks, and even outperforms much larger popular models such as e5-large-v2 or bge-base-en-v1.5. For more information about this model or how it was trained, head over to the announcement blogpost. ## Usage ### Installation This model was designed with the upcoming RAGatouille overhaul in mind. However, it's compatible with all recent ColBERT implementations! To use it, you can either use the Stanford ColBERT library, or RAGatouille. You can install both or either by simply running. If you're interested in using this model as a re-ranker (it vastly outperforms cross-encoders its size!), you can do so via the rerankers library: ### Rerankers ### RAGatouille ### Stanford ColBERT #### Indexing #### Querying #### Extracting Vectors Finally, if you want to extract individula vectors, you can use the model this way: ## Results ### Against single-vector models | 33M (1x) | 33M (1x) | **109M (3.3x)** | | **BEIR AVG** | **53.79** | 51.99 | 51.68 | 53.25 | | **FiQA2018** | **41.15** | 40.65 | 40.34 | 40.65 | | **HotpotQA** | **76.11** | 66.54 | 69.94 | 72.6 | | **MSMARCO** | **43.5** | 40.23 | 40.83 | 41.35 | | **NQ** | **59.1** | 50.9 | 50.18 | 54.15 | | **TRECCOVID** | **84.59** | 80.12 | 75.9 | 78.07 | | **ArguAna** | 50.09 | 57.59 | 59.55 | **63.61** | | **ClimateFEVER**| 33.07 | **35.2** | 31.84 | 31.17 | | **CQADupstackRetrieval** | 38.75 | 39.65 | 39.05 | **42.35** | | **DBPedia** | **45.58** | 41.02 | 40.03 | 40.77 | | **FEVER** | **90.96** | 87.13 | 86.64 | 86.29 | | **NFCorpus** | 37.3 | 34.92 | 34.3 | **37.39** | | **QuoraRetrieval** | 87.72 | 88.41 | 88.78 | **88.9** | | **SCIDOCS** | 18.42 | **21.82** | 20.52 | 21.73 | | **SciFact** | **74.77** | 72.22 | 71.28 | 74.04 | | **Touche2020** | 25.69 | 23.48 | **26.04** | 25.7 | ### Against ColBERTv2.0 | Dataset / Model | answerai-colbert-small-v1 | ColBERTv2.0 | |:-----------------|:-----------------------:|:------------:| | **BEIR AVG** | **53.79** | 50.02 | | **DBPedia** | **45.58** | 44.6 | | **FiQA2018** | **41.15** | 35.6 | | **NQ** | **59.1** | 56.2 | | **HotpotQA** | **76.11** | 66.7 | | **NFCorpus** | **37.3** | 33.8 | | **TRECCOVID** | **84.59** | 73.3 | | **Touche2020** | 25.69 | **26.3** | | **ArguAna** | **50.09** | 46.3 | | **ClimateFEVER**| **33.07** | 17.6 | | **FEVER** | **90.96** | 78.5 | | **QuoraRetrieval** | **87.72** | 85.2 | | **SCIDOCS** | **18.42** | 15.4 | | **SciFact** | **74.77** | 69.3 | ## Referencing We'll most likely eventually release a technical report. In the meantime, if you use this model or other models following the JaColBERTv2.5 recipe and would like to give us credit, please cite the JaColBERTv2.5 journal pre-print:",
    "model_explanation_gemini": "A compact, high-performing multi-vector model for efficient passage retrieval and reranking, outperforming larger models on benchmarks despite its small size (33M parameters)."
}