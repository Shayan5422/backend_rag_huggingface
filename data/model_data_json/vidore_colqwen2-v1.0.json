{
    "model_id": "vidore/colqwen2-v1.0",
    "downloads": 106016,
    "tags": [
        "colpali",
        "safetensors",
        "vidore-experimental",
        "vidore",
        "visual-document-retrieval",
        "en",
        "arxiv:2004.12832",
        "arxiv:2407.01449",
        "arxiv:2106.09685",
        "base_model:vidore/colqwen2-base",
        "base_model:finetune:vidore/colqwen2-base",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: apache-2.0 library_name: colpali base_model: vidore/colqwen2-base language: - en tags: - colpali - vidore-experimental - vidore pipeline_tag: visual-document-retrieval --- # ColQwen2: Visual Retriever based on Qwen2-VL-2B-Instruct with ColBERT strategy ### This is the base version trained with batch_size 256 instead of 32 for 5 epoch and with the updated pad token ColQwen2 is a model based on a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents from their visual features. It is a Qwen2-VL-2B extension that generates ColBERT- style multi-vector representations of text and images. It was introduced in the paper ColPali: Efficient Document Retrieval with Vision Language Models and first released in this repository <p align=\"center\"><img width=800 src=\" ## Version specificity This model takes dynamic image resolutions in input and does not resize them, changing their aspect ratio as in ColPali. Maximal resolution is set so that 768 image patches are created at most. Experiments show clear improvements with larger amounts of image patches, at the cost of memory requirements. This version is trained with . Data is the same as the ColPali data described in the paper. ## Model Training ### Dataset Our training dataset of 127,460 query-page pairs is comprised of train sets of openly available academic datasets (63%) and a synthetic dataset made up of pages from web-crawled PDF documents and augmented with VLM-generated (Claude-3 Sonnet) pseudo-questions (37%). Our training set is fully English by design, enabling us to study zero-shot generalization to non-English languages. We explicitly verify no multi-page PDF document is used both *ViDoRe* and in the train set to prevent evaluation contamination. A validation set is created with 2% of the samples to tune hyperparameters. *Note: Multilingual data is present in the pretraining corpus of the language model and most probably in the multimodal training.* ### Parameters All models are trained for 1 epoch on the train set. Unless specified otherwise, we train models in format, use low-rank adapters (LoRA) with and on the transformer layers from the language model, as well as the final randomly initialized projection layer, and use a optimizer. We train on an 8 GPU setup with data parallelism, a learning rate of 5e-5 with linear decay with 2.5% warmup steps, and a batch size of 32. ## Usage Make sure is installed from source or with a version superior to 0.3.4. version must be > 4.46.1. ## Limitations - **Focus**: The model primarily focuses on PDF-type documents and high-ressources languages, potentially limiting its generalization to other document types or less represented languages. - **Support**: The model relies on multi-vector retreiving derived from the ColBERT late interaction mechanism, which may require engineering efforts to adapt to widely used vector retrieval frameworks that lack native multi-vector support. ## License ColQwen2's vision language backbone model (Qwen2-VL) is under license. The adapters attached to the model are under MIT license. ## Contact - Manuel Faysse: manuel.faysse@illuin.tech - Hugues Sibille: hugues.sibille@illuin.tech - Tony Wu: tony.wu@illuin.tech ## Citation If you use any datasets or models from this organization in your research, please cite the original dataset as follows:",
    "model_explanation_gemini": "Generates multi-vector representations of text and images for efficient visual document retrieval, based on Qwen2-VL-2B with ColBERT-style indexing."
}