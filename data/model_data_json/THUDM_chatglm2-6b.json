{
    "model_id": "THUDM/chatglm2-6b",
    "downloads": 613020,
    "tags": [
        "transformers",
        "pytorch",
        "chatglm",
        "glm",
        "thudm",
        "custom_code",
        "zh",
        "en",
        "arxiv:2103.10360",
        "arxiv:2210.02414",
        "arxiv:1911.02150",
        "arxiv:2406.12793",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- language: - zh - en tags: - glm - chatglm - thudm --- # ChatGLM2-6B <p align=\"center\"> 💻 <a href=\" target=\"_blank\">Github Repo</a> • 🐦 <a href=\" target=\"_blank\">Twitter</a> • 📃 <a href=\" target=\"_blank\">[GLM@ACL 22]</a> <a href=\" target=\"_blank\">[GitHub]</a> • 📃 <a href=\" target=\"_blank\">[GLM-130B@ICLR 23]</a> <a href=\" target=\"_blank\">[GitHub]</a> <br> </p> <p align=\"center\"> 👋 Join our <a href=\" target=\"_blank\">Slack</a> and <a href=\" target=\"_blank\">WeChat</a> </p> <p align=\"center\"> 📍Experience the larger-scale ChatGLM model at <a href=\" </p> ## 介绍 ChatGLM**2**-6B 是开源中英双语对话模型 ChatGLM-6B 的第二代版本，在保留了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM**2**-6B 引入了如下新特性： 1. **更强大的性能**：基于 ChatGLM 初代模型的开发经验，我们全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B 使用了 GLM 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，评测结果显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。 2. **更长的上下文**：基于 FlashAttention 技术，我们将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。但当前版本的 ChatGLM2-6B 对单轮超长文档的理解能力有限，我们会在后续迭代升级中着重进行优化。 3. **更高效的推理**：基于 Multi-Query Attention 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。 4. **更开放的协议**：ChatGLM2-6B 权重对学术研究**完全开放**，在填写问卷进行登记后**亦允许免费商业使用**。 ChatGLM**2**-6B is the second-generation version of the open-source bilingual (Chinese-English) chat model ChatGLM-6B. It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features: 1. **Stronger Performance**: Based on the development experience of the first-generation ChatGLM model, we have fully upgraded the base model of ChatGLM2-6B. ChatGLM2-6B uses the hybrid objective function of GLM, and has undergone pre-training with 1.4T bilingual tokens and human preference alignment training. The evaluation results show that, compared to the first-generation model, ChatGLM2-6B has achieved substantial improvements in performance on datasets like MMLU (+23%), CEval (+33%), GSM8K (+571%), BBH (+60%), showing strong competitiveness among models of the same size. 2. **Longer Context**: Based on FlashAttention technique, we have extended the context length of the base model from 2K in ChatGLM-6B to 32K, and trained with a context length of 8K during the dialogue alignment, allowing for more rounds of dialogue. However, the current version of ChatGLM2-6B has limited understanding of single-round ultra-long documents, which we will focus on optimizing in future iterations. 3. **More Efficient Inference**: Based on Multi-Query Attention technique, ChatGLM2-6B has more efficient inference speed and lower GPU memory usage: under the official implementation, the inference speed has increased by 42% compared to the first generation; under INT4 quantization, the dialogue length supported by 6G GPU memory has increased from 1K to 8K. 4. **More Open License**: ChatGLM2-6B weights are **completely open** for academic research, and **free commercial use** is also allowed after completing the questionnaire. ## 软件依赖 ## 代码调用 可以通过如下代码调用 ChatGLM-6B 模型来生成对话： 关于更多的使用说明，包括如何运行命令行和网页版本的 DEMO，以及使用模型量化以节省显存，请参考我们的 Github Repo。 For more instructions, including how to run CLI and web demos, and model quantization, please refer to our Github Repo. ## Change Log * v1.0 ## 协议 本仓库的代码依照 Apache-2.0 协议开源，ChatGLM2-6B 模型的权重的使用则需要遵循 Model License。 ## 引用 如果你觉得我们的工作有帮助的话，请考虑引用下列论文。 If you find our work helpful, please consider citing the following paper.",
    "model_explanation_gemini": "An open-source bilingual (Chinese-English) chat model optimized for smooth dialogue, offering stronger performance, longer context handling, and efficient inference while allowing academic and commercial use."
}