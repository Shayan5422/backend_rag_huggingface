{
    "model_id": "pyannote/speaker-diarization",
    "downloads": 864999,
    "tags": [
        "pyannote-audio",
        "pyannote",
        "pyannote-audio-pipeline",
        "audio",
        "voice",
        "speech",
        "speaker",
        "speaker-diarization",
        "speaker-change-detection",
        "voice-activity-detection",
        "overlapped-speech-detection",
        "automatic-speech-recognition",
        "dataset:ami",
        "dataset:dihard",
        "dataset:voxconverse",
        "dataset:aishell",
        "dataset:repere",
        "dataset:voxceleb",
        "arxiv:2012.01477",
        "arxiv:2110.07058",
        "arxiv:2005.08072",
        "license:mit",
        "region:us"
    ],
    "description": "--- tags: - pyannote - pyannote-audio - pyannote-audio-pipeline - audio - voice - speech - speaker - speaker-diarization - speaker-change-detection - voice-activity-detection - overlapped-speech-detection - automatic-speech-recognition datasets: - ami - dihard - voxconverse - aishell - repere - voxceleb license: mit extra_gated_prompt: \"The collected information will help acquire a better knowledge of pyannote.audio userbase and help its maintainers apply for grants to improve it further. If you are an academic researcher, please cite the relevant papers in your own publications using the model. If you work for a company, please consider contributing back to pyannote.audio development (e.g. through unrestricted gifts). We also provide scientific consulting services around speaker diarization and machine listening.\" extra_gated_fields: Company/university: text Website: text I plan to use this model for (task, type of audio data, etc): text --- Using this open-source model in production? Consider switching to pyannoteAI for better and faster options. # ðŸŽ¹ Speaker diarization Relies on pyannote.audio 2.1.1: see installation instructions. ## TL;DR ## Advanced usage In case the number of speakers is known in advance, one can use the option: One can also provide lower and/or upper bounds on the number of speakers using and options: ## Benchmark ### Real-time factor Real-time factor is around 2.5% using one Nvidia Tesla V100 SXM2 GPU (for the neural inference part) and one Intel Cascade Lake 6248 CPU (for the clustering part). In other words, it takes approximately 1.5 minutes to process a one hour conversation. ### Accuracy This pipeline is benchmarked on a growing collection of datasets. Processing is fully automatic: * no manual voice activity detection (as is sometimes the case in the literature) * no manual number of speakers (though it is possible to provide it to the pipeline) * no fine-tuning of the internal models nor tuning of the pipeline hyper-parameters to each dataset ... with the least forgiving diarization error rate (DER) setup (named *\"Full\"* in this paper): * no forgiveness collar * evaluation of overlapped speech | Benchmark | DER% | FA% | Miss% | Conf% | Expected output | File-level evaluation | | ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------- | --------------------------- | ---------------------------------- | ----------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | | AISHELL-4 | 14.09 | 5.17 | 3.27 | 5.65 | RTTM | eval | | Albayzin (*RTVE 2022*) | 25.60 | 5.58 | 6.84 | 13.18 | RTTM | eval | | AliMeeting (*channel 1*) | 27.42 | 4.84 | 14.00 | 8.58 | RTTM | eval | | AMI (*headset mix,* *only_words*) | 18.91 | 4.48 | 9.51 | 4.91 | RTTM | eval | | AMI (*array1, channel 1,* *only_words)* | 27.12 | 4.11 | 17.78 | 5.23 | RTTM | eval | | CALLHOME (*part2*) | 32.37 | 6.30 | 13.72 | 12.35 | RTTM | eval | | DIHARD 3 (*Full*) | 26.94 | 10.50 | 8.41 | 8.03 | RTTM | eval | | Ego4D *v1 (validation)* | 63.99 | 3.91 | 44.42 | 15.67 | RTTM | eval | | REPERE (*phase 2*) | 8.17 | 2.23 | 2.49 | 3.45 | RTTM | eval | | This American Life | 20.82 | 2.03 | 11.89 | 6.90 | RTTM | eval | | VoxConverse (*v0.3*) | 11.24 | 4.42 | 2.88 | 3.94 | RTTM | eval | ## Technical report This report describes the main principles behind version of pyannote.audio speaker diarization pipeline. It also provides recipes explaining how to adapt the pipeline to your own set of annotated data. In particular, those are applied to the above benchmark and consistently leads to significant performance improvement over the above out-of-the-box performance. ## Citations"
}