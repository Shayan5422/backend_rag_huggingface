{
    "model_id": "JamAndTeaStudios/DeepSeek-R1-Distill-Qwen-32B-FP8-Dynamic",
    "downloads": 230793,
    "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "chat",
        "conversational",
        "en",
        "base_model:deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "base_model:quantized:deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "license:mit",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "compressed-tensors",
        "region:us"
    ],
    "description": "--- license: mit language: - en pipeline_tag: text-generation base_model: - deepseek-ai/DeepSeek-R1-Distill-Qwen-32B tags: - chat library_name: transformers --- # Model Overview - **Model Optimizations:** - **Weight quantization:** FP8 - **Activation quantization:** FP8 - **Release Date:** 1/28/2025 Quantized version of deepseek-ai/DeepSeek-R1-Distill-Qwen-32B to FP8 data type, ready for inference with SGLang >= 0.3 or vLLM >= 0.5.2. This optimization reduces the number of bits per parameter from 16 to 8, reducing the disk size and GPU memory requirements by approximately 50%. Only the weights and activations of the linear operators within transformers blocks are quantized. ## Deployment ### Use with SGLang ## Creation This model was created with llm-compressor by running the code snippet below. <details> <summary>Model Creation Code</summary> </details> ## Evaluation TBA ## Play Retail Mage !image/png Retail Mage (Steam) is an immersive sim that uses online LLM inference in almost all features in the gameplay! Reviews “A true to life experience detailing how customer service really works.” 10/10 – kpolupo “I enjoyed how many things were flammable in the store.” 5/5 – mr_srsbsns “I've only known that talking little crow plushie in MageMart for a day and a half but if anything happened to him I would petrify everyone in this store and then myself.” 7/7 – neondenki",
    "model_explanation_gemini": "An FP8-quantized version of DeepSeek-R1-Distill-Qwen-32B optimized for reduced disk size and GPU memory usage, designed for efficient text generation inference with SGLang or vLLM."
}