{
    "model_id": "deepset/bert-large-uncased-whole-word-masking-squad2",
    "downloads": 204585,
    "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "safetensors",
        "bert",
        "question-answering",
        "en",
        "dataset:squad_v2",
        "license:cc-by-4.0",
        "model-index",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- language: en license: cc-by-4.0 datasets: - squad_v2 model-index: - name: deepset/bert-large-uncased-whole-word-masking-squad2 results: - task: type: question-answering name: Question Answering dataset: name: squad_v2 type: squad_v2 config: squad_v2 split: validation metrics: - type: exact_match value: 80.8846 name: Exact Match verified: true verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiY2E5ZGNkY2ExZWViZGEwNWE3OGRmMWM2ZmE4ZDU4ZDQ1OGM3ZWE0NTVmZjFmYmZjZmJmNjJmYTc3NTM3OTk3OSIsInZlcnNpb24iOjF9.aSblF4ywh1fnHHrN6UGL392R5KLaH3FCKQlpiXo_EdQ4XXEAENUCjYm9HWDiFsgfSENL35GkbSyz_GAhnefsAQ - type: f1 value: 83.8765 name: F1 verified: true verifyToken: eyJhbGciOiJFZERTQSIsInR5cCI6IkpXVCJ9.eyJoYXNoIjoiNGFlNmEzMTk2NjRkNTI3ZTk3ZTU1NWNlYzIyN2E0ZDFlNDA2ZjYwZWJlNThkMmRmMmE0YzcwYjIyZDM5NmRiMCIsInZlcnNpb24iOjF9.-rc2_Bsp_B26-o12MFYuAU0Ad2Hg9PDx7Preuk27WlhYJDeKeEr32CW8LLANQABR3Mhw2x8uTYkEUrSDMxxLBw - task: type: question-answering name: Question Answering dataset: name: squad type: squad config: plain_text split: validation metrics: - type: exact_match value: 85.904 name: Exact Match - type: f1 value: 92.586 name: F1 - task: type: question-answering name: Question Answering dataset: name: adversarial_qa type: adversarial_qa config: adversarialQA split: validation metrics: - type: exact_match value: 28.233 name: Exact Match - type: f1 value: 41.170 name: F1 - task: type: question-answering name: Question Answering dataset: name: squad_adversarial type: squad_adversarial config: AddOneSent split: validation metrics: - type: exact_match value: 78.064 name: Exact Match - type: f1 value: 83.591 name: F1 - task: type: question-answering name: Question Answering dataset: name: squadshifts amazon type: squadshifts config: amazon split: test metrics: - type: exact_match value: 65.615 name: Exact Match - type: f1 value: 80.733 name: F1 - task: type: question-answering name: Question Answering dataset: name: squadshifts new_wiki type: squadshifts config: new_wiki split: test metrics: - type: exact_match value: 81.570 name: Exact Match - type: f1 value: 91.199 name: F1 - task: type: question-answering name: Question Answering dataset: name: squadshifts nyt type: squadshifts config: nyt split: test metrics: - type: exact_match value: 83.279 name: Exact Match - type: f1 value: 91.090 name: F1 - task: type: question-answering name: Question Answering dataset: name: squadshifts reddit type: squadshifts config: reddit split: test metrics: - type: exact_match value: 69.305 name: Exact Match - type: f1 value: 82.405 name: F1 --- # bert-large-uncased-whole-word-masking-squad2 for Extractive QA This is a berta-large model, fine-tuned using the SQuAD2.0 dataset for the task of question answering. ## Overview **Language model:** bert-large **Language:** English **Downstream-task:** Extractive QA **Training data:** SQuAD 2.0 **Eval data:** SQuAD 2.0 **Code:** See an example extractive QA pipeline built with Haystack ## Usage ### In Haystack Haystack is an AI orchestration framework to build customizable, production-ready LLM applications. You can use this model in Haystack to do extractive question answering on documents. To load and run the model with Haystack: For a complete example with an extractive question answering pipeline that scales over many documents, check out the corresponding Haystack tutorial. ### In Transformers ## About us <div class=\"grid lg:grid-cols-2 gap-x-4 gap-y-3\"> <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\"> <img alt=\"\" src=\" class=\"w-40\"/> </div> <div class=\"w-full h-40 object-cover mb-2 rounded-lg flex items-center justify-center\"> <img alt=\"\" src=\" class=\"w-40\"/> </div> </div> deepset is the company behind the production-ready open-source AI framework Haystack. Some of our other work: - Distilled roberta-base-squad2 (aka \"tinyroberta-squad2\") - German BERT, GermanQuAD and GermanDPR, German embedding model - deepset Cloud, deepset Studio ## Get in touch and join the Haystack community <p>For more info on Haystack, visit our <strong><a href=\" repo and <strong><a href=\" We also have a <strong><a class=\"h-7\" href=\" community open to everyone!</a></strong></p> Twitter | LinkedIn | Discord | GitHub Discussions | Website | YouTube By the way: we're hiring!",
    "model_explanation_gemini": "A BERT-large model fine-tuned on SQuAD2.0 for extractive question answering, designed to answer questions based on given text passages."
}