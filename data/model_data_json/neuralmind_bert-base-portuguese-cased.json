{
    "model_id": "neuralmind/bert-base-portuguese-cased",
    "downloads": 219018,
    "tags": [
        "transformers",
        "pytorch",
        "tf",
        "jax",
        "bert",
        "fill-mask",
        "pt",
        "dataset:brWaC",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- language: pt license: mit tags: - bert - pytorch datasets: - brWaC --- # BERTimbau Base (aka \"bert-base-portuguese-cased\") !Bert holding a berimbau ## Introduction BERTimbau Base is a pretrained BERT model for Brazilian Portuguese that achieves state-of-the-art performances on three downstream NLP tasks: Named Entity Recognition, Sentence Textual Similarity and Recognizing Textual Entailment. It is available in two sizes: Base and Large. For further information or requests, please go to BERTimbau repository. ## Available models | Model | Arch. | #Layers | #Params | | ---------------------------------------- | ---------- | ------- | ------- | | | BERT-Base | 12 | 110M | | | BERT-Large | 24 | 335M | ## Usage ### Masked language modeling prediction example ### For BERT embeddings ## Citation If you use our work, please cite:",
    "model_explanation_gemini": "Pretrained BERT model for Brazilian Portuguese optimized for named entity recognition, sentence similarity, and textual entailment tasks."
}