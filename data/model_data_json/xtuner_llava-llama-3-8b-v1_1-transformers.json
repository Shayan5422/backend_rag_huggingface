{
    "model_id": "xtuner/llava-llama-3-8b-v1_1-transformers",
    "downloads": 447968,
    "tags": [
        "xtuner",
        "safetensors",
        "llava",
        "image-text-to-text",
        "conversational",
        "dataset:Lin-Chen/ShareGPT4V",
        "region:us"
    ],
    "description": "--- datasets: - Lin-Chen/ShareGPT4V pipeline_tag: image-text-to-text library_name: xtuner --- <div align=\"center\"> <img src=\" width=\"600\"/> | LLaVA-Mix (665K) | | LLaVA-Llama-3-8B | CLIP-L | MLP | 336 | Frozen LLM, Frozen ViT | Full LLM, LoRA ViT | LLaVA-PT (558K) | LLaVA-Mix (665K) | | LLaVA-Llama-3-8B-v1.1 | CLIP-L | MLP | 336 | Frozen LLM, Frozen ViT | Full LLM, LoRA ViT | ShareGPT4V-PT (1246K) | InternVL-SFT (1268K) | ## Results <div align=\"center\"> <img src=\" alt=\"Image\" width=500\" /> </div> | Model | MMBench Test (EN) | MMBench Test (CN) | CCBench Dev | MMMU Val | SEED-IMG | AI2D Test | ScienceQA Test | HallusionBench aAcc | POPE | GQA | TextVQA | MME | MMStar | | :-------------------- | :---------------: | :---------------: | :---------: | :-------: | :------: | :-------: | :------------: | :-----------------: | :--: | :--: | :-----: | :------: | :----: | | LLaVA-v1.5-7B | 66.5 | 59.0 | 27.5 | 35.3 | 60.5 | 54.8 | 70.4 | 44.9 | 85.9 | 62.0 | 58.2 | 1511/348 | 30.3 | | LLaVA-Llama-3-8B | 68.9 | 61.6 | 30.4 | 36.8 | 69.8 | 60.9 | 73.3 | 47.3 | 87.2 | 63.5 | 58.0 | 1506/295 | 38.2 | | LLaVA-Llama-3-8B-v1.1 | 72.3 | 66.4 | 31.6 | 36.8 | 70.1 | 70.0 | 72.9 | 47.7 | 86.4 | 62.6 | 59.0 | 1469/349 | 45.1 | ## QuickStart ### Chat by ### Chat by pure ### Reproduce Please refer to docs. ## Citation",
    "model_explanation_gemini": "An image-text-to-text model based on LLaMA-3-8B, designed for multimodal tasks like visual question answering and image understanding through frozen LLM and ViT components with LoRA adaptation."
}