{
    "model_id": "facebook/metaclip-b16-fullcc2.5b",
    "downloads": 87437,
    "tags": [
        "transformers",
        "pytorch",
        "clip",
        "zero-shot-image-classification",
        "vision",
        "metaclip",
        "arxiv:2309.16671",
        "arxiv:2103.00020",
        "license:cc-by-nc-4.0",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: cc-by-nc-4.0 tags: - vision - metaclip widget: - src: candidate_labels: playing music, playing sports example_title: Cat & Dog --- # MetaCLIP model, base-sized version, patch resolution 16 MetaCLIP model applied to 2.5 billion data points of CommonCrawl (CC). It was introduced in the paper Demystifying CLIP Data by Xu et al. and first released in this repository. Disclaimer: The team releasing MetaCLIP did not write a model card for this model so this model card has been written by the Hugging Face team. ## Model description The Demystifying CLIP Data paper aims to reveal CLIPâ€™s method around training data curation. OpenAI never open-sourced code regarding their data preparation pipeline. <img src=\" alt=\"drawing\" width=\"600\"/> <small> CLIP high-level overview. Taken from the <a href=\" paper</a>. </small> ## Intended uses & limitations You can use the raw model for linking images with text in a shared embedding space. This enables things like zero-shot image classification, text-based image retrieval, image-based text retrieval, etc. ### How to use We refer to the docs. Just replace the names of the models on the hub. ### BibTeX entry and citation info",
    "model_explanation_gemini": "Links images with text in a shared embedding space for tasks like zero-shot image classification and text-based image retrieval."
}