{
    "model_id": "nvidia/Aegis-AI-Content-Safety-LlamaGuard-Defensive-1.0",
    "downloads": 1102006,
    "tags": [
        "peft",
        "safetensors",
        "text-classification",
        "en",
        "dataset:nvidia/Aegis-AI-Content-Safety-Dataset-1.0",
        "arxiv:2307.09288",
        "arxiv:2404.05993",
        "license:llama2",
        "region:us"
    ],
    "description": "--- license: llama2 datasets: - nvidia/Aegis-AI-Content-Safety-Dataset-1.0 language: - en metrics: - f1 library_name: peft pipeline_tag: text-classification --- # Model Card ## License The use of this model is governed by the Llama 2 Community License Agreement. ## Model Details Aegis-AI-Content-Safety-LlamaGuard-LLM-Defensive-1.0 is a LLM content safety model. It is a parameter efficient instruction tuned version of Llama Guard based on Llama2-7B trained on Nvidia's content safety dataset Aegis Content Safety Dataset covering Nvidia's broad taxonomy of 13 critical safety risk categories. Paper Details: Aegis Content Moderation ### Model Description The Aegis-AI-Content-Safety-LlamaGuard-LLM-Defensive-1.0 model involves the following: 1. System instruction including the safety taxonomy, a safety policy with inclusions and, exclusions. 2. The system prompt instructs the LLM to moderate user prompt, partial dialog or full dialog. 3. The LLM response is a string which can be either safe or unsafe. If the string generated by the LLM is \"unsafe\", on a new line, the category ID of violation is output by the LLM based on the policy in the system prompt. 4. Novel safety risk categories and policy can be provided in the instruction for the model to categorize using the novel taxonomy and policy. 5. The safety taxonomy and policy used to train the models contain 13 critically unsafe risk categories, a safe category and a \"needs caution\" category. 6. Internally annotated dataset called Aegis-AI-Content-Safety-Dataset-1.0 of approximately 11,000 prompts and responses are used to instruction tune the model. Annotations are at dialog level not per turn. We have since collected in total 30,000 annotations on a further expanded taxonomy and future versions of the models will be trained on the full set. The annotations are at dialog level instead of per-turn level. 7. Model is instruction tuned with safety instruction, with the LLM behaving as a classifier in this setting. PLEASE NOTE: Model has only been trained to perform prompt classification since the annotations were not available at turn level. If you wish to use the model for response classification, use the template as provided below. # Prompt used for training and evaluation: **Output (Model Response)** - **Developed by:** Shaona Ghosh, Nvidia - **Model type:** Instruction tuned LLama2-7B - **License:** Llama 2 - **Finetuned from model:** Llama Guard ## Uses Ethical use: Technology can have a profound impact on people and the world, and NVIDIA is committed to enabling trust and transparency in AI development. NVIDIA encourages users to adopt principles of AI ethics and trustworthiness to guide your business decisions by following the guidelines in the Llama 2 Community License Agreement. ### Direct Use - The Aegis-AI-Content-Safety-LlamaGuard-LLM-Defensive-1.0 model is for users who wants to safeguard or evaluate a general purpose LLM's generated content Model and dataset restrictions: The Principle of least privilege (PoLP) is applied limiting access for dataset generation and model development. Restrictions enforce dataset access during training, and dataset license constraints adhered to. ### Downstream Use - Alternatively, the model can be used for performing toxicity classification for any text content such as pre-training data not exclusively limited to human-LLM interaction data - The model can be finetuned further with custom safety policy and taxonomies. - Different adapter weights (used in conjunction with this model) can be used to enforce different safety tolerance. ## Bias, Risks, and Limitations Given the nature of the work, the model has been trained on critically unsafe data that includes social biases to be able to categorize the safety risks based on a broad safety risk taxonomy. However, - Even though we have performed exhaustive evaluation, occasionally, the model can make errors in predicting the unsafe category. - Even though, we have internally red teamed the model (please see paper for details), the safety guardrails of the model can be bypassed by adversarial prompts and the underlying LLM may be prompted to generate unsafe text. ### Bias Field | Response :---------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------- Participation considerations from adversely impacted groups (protected classes) in model design and testing: | None of the Above Measures taken to mitigate against unwanted bias: | None of the Above ### Privacy Field | Response :----------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------- Generatable or reverse engineerable personally-identifiable information (PII)? | None Was consent obtained for any PII used? | Not Applicable PII used to create this model? | None Known How often is dataset reviewed? | During dataset creation, model training, evaluation and before release Is a mechanism in place to honor data subject right of access or deletion of personal data? | Not Applicable If PII collected for the development of the model, was it collected directly by NVIDIA? | Not Applicable If PII collected for the development of the model by NVIDIA, do you maintain or have access to disclosures made to data subjects? | Not Applicable If PII collected for the development of this AI model, was it minimized to only what was required? | Not Applicable Is there provenance for all datasets used in training? | Yes Does data labeling (annotation, metadata) comply with privacy laws? | Yes Is data compliant with data subject requests for data correction or removal, if such a request was made? | Not Applicable ### Recommendations We recommend users to monitor for the above risks before deploying the models. If you notice any concerns, please report to us immediately. ## How to Get Started with the Model - Download the original Llama Guard weights from Llama Guard after requesting access. - Use transformers PEFT library for loading the adapter weights from this repository. - Format the prompt using the functions below: ## How To Use in NVIDIA NeMo Curator NeMo Curator improves generative AI model accuracy by processing text, image, and video data at scale for training and customization. It also provides pre-built pipelines for generating synthetic data to customize and evaluate generative AI systems. The inference code for this model is available through the NeMo Curator GitHub repository. Check out this example notebook to get started. ## Training Details ### Training Data The model has been trained on Nvidia's Aegis Content Safety Dataset * Human Prompts from Anthropic RLHF harmless dataset Anthropic RLHF * LLM response generated from Mistral-7B-v0.1 Mistral-7B-v0.1 ***Labeling Method by dataset*** * Human **Properties** Trained on approximately 10,800 user prompts, user prompts and LLM response single turn, user prompts and LLM response muliple turns. #### Training Hyperparameters * rank 16 * alpha 32 * Num of nodes 1 * Num of GPUs per node 8 * Learning rate 1e-06 ### Training Procedure We use the PEFT library from Hugging Face and the training and validation code from the Llama recipes repository. We use FSDP during training. - **Training regime:** fp16 ## Evaluation ### Testing Data, Factors & Metrics #### Testing Data The model has been evaluated on the following benchmarks: * Test partition of Nvidia's content safety dataset Aegis Content Safety Dataset * Toxic Chat Dataset * Open AI Moderation Dataset * SimpleSafetyTests Benchmark #### Metrics We report F1 and AUPRC scores for the model on the evaluation benchmarks. ### Results on Aegis Content Safety Test Set Model | AUPRC | F1 | ------------ |:-----------: |-----------: | Llama Guard Base |0.930 |0.62 | OpenAI Mod API |0.895 |0.34 | Perspective API |0.860 |0.24 | Llama Guard Defensive |0.941 |0.85 | ### Results on Toxic Chat Dataset Model | AUPRC | F1 | ------------ |:-----------: |-----------: | Llama Guard Base |0.664 |0.58 | OpenAI Mod API |0.588 |- | Perspective API |0.532 |- | Llama Guard Defensive |0.699 |0.64 | ### Results on Open AI Moderation Dataset Model | AUPRC | F1 | ------------ |:-----------: |-----------: | Llama Guard Base |0.845 |0.76 | OpenAI Mod API |0.856 |- | Perspective API |0.787 |- | Llama Guard Defensive |0.844 |0.68 | ### Results on Simple Safety Tests Benchmark Model | Accuracy | ------------ |:-----------: | Llama Guard Base |87% | Perspective API |72% | GPT4 |89% | Llama Guard Defensive |100% | ### Compute Infrastructure #### Supported Hardware: H100, A100 80GB, A100 40GB ## Citation **BibTeX:** ## Model Card Contact shaonag@nvidia.com",
    "model_explanation_gemini": "Classifies text content as safe or unsafe based on 13 critical safety risk categories, providing violation details when unsafe."
}