{
    "model_id": "prov-gigapath/prov-gigapath",
    "downloads": 187761,
    "tags": [
        "timm",
        "pytorch",
        "vision",
        "medical",
        "image-feature-extraction",
        "license:apache-2.0",
        "region:us"
    ],
    "description": "--- license: apache-2.0 tags: - vision - medical pipeline_tag: image-feature-extraction library_name: timm --- # Prov-GigaPath ## A whole-slide foundation model for digital pathology from real-world data [[]]( [] [] [] Hanwen Xu*, Naoto Usuyama*, Jaspreet Bagga, Sheng Zhang, Rajesh Rao, Tristan Naumann, Cliff Wong, Zelalem Gero, Javier González, Yu Gu, Yanbo Xu, Mu Wei, Wenhui Wang, Shuming Ma, Furu Wei, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Jaylen Rosemon, Tucker Bower, Soohee Lee, Roshanthi Weerasinghe, Bill J. Wright, Ari Robicsek, Brian Piening, Carlo Bifulco, Sheng Wang, Hoifung Poon (*Equal Contribution) Tile the whole slide into N image tiles, with the coordinates of each tile. (2) Get the embeddings for each tile using our tile encoder. (3) Pass the N image tile embeddings and their coordinates into the slide encoder, to get slide level representations. ### Inference with the tile encoder First, load GigaPath tile encoder: Running inference to extract tile level features: ### Inference with the slide encoder To inference with our slide encoder, we need both the tile embeddings and their coordinates as input. First, let's load the GigaPath slide encoder: Run the inference to get the slide level embeddings: ## Fine-tuning ### Tile-Level Linear Probing Example Using PCam Dataset For your convenience, we provide the pre-extracted embeddings for the PCam dataset. You can download them from the link below. Note that the file size is 2GB. There is no need to unzip this file. To run the fine-tuning experiment, execute the following script: ### Slide-Level Fine-Tuning Example Using PANDA Dataset For your convenience, we provide the pre-extracted embeddings for the PANDA dataset. You can download them from the link below. Note that the file size is 32GB. Please unzip this file. To run the fine-tuning experiment, execute the following script: ## Sample Data Download A sample de-identified subset of the Prov-Path data can be accessed from these links [1, 2]. ## Model Uses ### Intended Use The data, code, and model checkpoints are intended to be used solely for (I) future research on pathology foundation models and (II) reproducibility of the experimental results reported in the reference paper. The data, code, and model checkpoints are not intended to be used in clinical care or for any clinical decision-making purposes. ### Primary Intended Use The primary intended use is to support AI researchers reproducing and building on top of this work. GigaPath should be helpful for exploring pre-training, and encoding of digital pathology slides data. ### Out-of-Scope Use **Any** deployed use case of the model --- commercial or otherwise --- is out of scope. Although we evaluated the models using a broad set of publicly-available research benchmarks, the models and evaluations are intended *for research use only* and not intended for deployed use cases. ## Usage and License Notices The model is not intended or made available for clinical use as a medical device, clinical support, diagnostic tool, or other technology intended to be used in the diagnosis, cure, mitigation, treatment, or prevention of disease or other conditions. The model is not designed or intended to be a substitute for professional medical advice, diagnosis, treatment, or judgment and should not be used as such. All users are responsible for reviewing the output of the developed model to determine whether the model meets the user’s needs and for validating and evaluating the model before any clinical use. ## Acknowledgements We would like to express our gratitude to the authors and developers of the exceptional repositories that this project is built upon: DINOv2, MAE, Timm, and TorchScale. Their contributions have been invaluable to our work. ## Citation If you find Prov-GigaPath useful for your your research and applications, please cite using this BibTeX:"
}