{
    "model_id": "Qwen/QwQ-32B-Preview",
    "downloads": 97541,
    "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "chat",
        "conversational",
        "en",
        "arxiv:2407.10671",
        "base_model:Qwen/Qwen2.5-32B-Instruct",
        "base_model:finetune:Qwen/Qwen2.5-32B-Instruct",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: apache-2.0 license_link: language: - en base_model: Qwen/Qwen2.5-32B-Instruct tags: - chat library_name: transformers --- # QwQ-32B-Preview <a href=\" target=\"_blank\" style=\"margin: 2px;\"> <img alt=\"Chat\" src=\" style=\"display: inline-block; vertical-align: middle;\"/> </a> ## Introduction **QwQ-32B-Preview** is an experimental research model developed by the Qwen Team, focused on advancing AI reasoning capabilities. As a preview release, it demonstrates promising analytical abilities while having several important limitations: 1. **Language Mixing and Code-Switching**: The model may mix languages or switch between them unexpectedly, affecting response clarity. 2. **Recursive Reasoning Loops**: The model may enter circular reasoning patterns, leading to lengthy responses without a conclusive answer. 3. **Safety and Ethical Considerations**: The model requires enhanced safety measures to ensure reliable and secure performance, and users should exercise caution when deploying it. 4. **Performance and Benchmark Limitations**: The model excels in math and coding but has room for improvement in other areas, such as common sense reasoning and nuanced language understanding. **Specification**: - Type: Causal Language Models - Training Stage: Pretraining & Post-training - Architecture: transformers with RoPE, SwiGLU, RMSNorm, and Attention QKV bias - Number of Parameters: 32.5B - Number of Paramaters (Non-Embedding): 31.0B - Number of Layers: 64 - Number of Attention Heads (GQA): 40 for Q and 8 for KV - Context Length: Full 32,768 tokens For more details, please refer to our blog. You can also check Qwen2.5 GitHub, and Documentation. ## Requirements The code of Qwen2.5 has been in the latest Hugging face and we advise you to use the latest version of . With , you will encounter the following error: ## Quickstart Here provides a code snippet with to show you how to load the tokenizer and model and how to generate contents. ## Citation If you find our work helpful, feel free to give us a cite.",
    "model_explanation_gemini": "QwQ-32B-Preview is an experimental 32.5B-parameter causal language model focused on advancing AI reasoning, particularly in math and coding, while exhibiting limitations in language mixing, recursive loops, and safety measures."
}