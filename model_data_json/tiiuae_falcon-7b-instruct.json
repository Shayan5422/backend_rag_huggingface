{
    "model_id": "tiiuae/falcon-7b-instruct",
    "downloads": 179698,
    "tags": [
        "transformers",
        "pytorch",
        "coreml",
        "safetensors",
        "falcon",
        "text-generation",
        "conversational",
        "custom_code",
        "en",
        "dataset:tiiuae/falcon-refinedweb",
        "arxiv:2205.14135",
        "arxiv:1911.02150",
        "arxiv:2005.14165",
        "arxiv:2104.09864",
        "arxiv:2306.01116",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- datasets: - tiiuae/falcon-refinedweb language: - en inference: true new_version: tiiuae/falcon-11B widget: - text: \"Hey Falcon! Any recommendations for my holidays in Abu Dhabi?\" example_title: \"Abu Dhabi Trip\" - text: \"What's the Everett interpretation of quantum mechanics?\" example_title: \"Q/A: Quantum & Answers\" - text: \"Give me a list of the top 10 dive sites you would recommend around the world.\" example_title: \"Diving Top 10\" - text: \"Can you tell me more about deep-water soloing?\" example_title: \"Extreme sports\" - text: \"Can you write a short tweet about the Apache 2.0 release of our latest AI model, Falcon LLM?\" example_title: \"Twitter Helper\" - text: \"What are the responsabilities of a Chief Llama Officer?\" example_title: \"Trendy Jobs\" license: apache-2.0 --- # âœ¨ Falcon-7B-Instruct **Falcon-7B-Instruct is a 7B parameters causal decoder-only model built by TII based on Falcon-7B and finetuned on a mixture of chat/instruct datasets. It is made available under the Apache 2.0 license.** *Paper coming soon ðŸ˜Š.* ðŸ¤— To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF! ## Why use Falcon-7B-Instruct? * **You are looking for a ready-to-use chat/instruct model based on Falcon-7B.** * **Falcon-7B is a strong base model, outperforming comparable open-source models** (e.g., MPT-7B, StableLM, RedPajama etc.), thanks to being trained on 1,500B tokens of RefinedWeb enhanced with curated corpora. See the OpenLLM Leaderboard. * **It features an architecture optimized for inference**, with FlashAttention (Dao et al., 2022) and multiquery (Shazeer et al., 2019). ðŸ’¬ **This is an instruct model, which may not be ideal for further finetuning.** If you are interested in building your own instruct/chat model, we recommend starting from Falcon-7B. ðŸ”¥ **Looking for an even more powerful model?** Falcon-40B-Instruct is Falcon-7B-Instruct's big brother! ðŸ’¥ **Falcon LLMs require PyTorch 2.0 for use with !** For fast inference with Falcon, check-out Text Generation Inference! Read more in this blogpost. You will need **at least 16GB of memory** to swiftly run inference with Falcon-7B-Instruct. # Model Card for Falcon-7B-Instruct ## Model Details ### Model Description - **Developed by:** - **Model type:** Causal decoder-only; - **Language(s) (NLP):** English and French; - **License:** Apache 2.0; - **Finetuned from model:** Falcon-7B. ### Model Source - **Paper:** *coming soon*. ## Uses ### Direct Use Falcon-7B-Instruct has been finetuned on a mixture of instruct and chat datasets. ### Out-of-Scope Use Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. ## Bias, Risks, and Limitations Falcon-7B-Instruct is mostly trained on English data, and will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online. ### Recommendations We recommend users of Falcon-7B-Instruct to develop guardrails and to take appropriate precautions for any production use. ## How to Get Started with the Model ## Training Details ### Training Data Falcon-7B-Instruct was finetuned on a 250M tokens mixture of instruct/chat datasets. | **Data source** | **Fraction** | **Tokens** | **Description** | |--------------------|--------------|------------|-----------------------------------| | Bai ze | 65% | 164M | chat | | GPT4All | 25% | 62M | instruct | | GPTeacher | 5% | 11M | instruct | | RefinedWeb-English | 5% | 13M | massive web crawl | The data was tokenized with the Falcon-7B/40B tokenizer. ## Evaluation *Paper coming soon.* See the OpenLLM Leaderboard for early results. Note that this model variant is not optimized for NLP benchmarks. ## Technical Specifications For more information about pretraining, see Falcon-7B. ### Model Architecture and Objective Falcon-7B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token). The architecture is broadly adapted from the GPT-3 paper (Brown et al., 2020), with the following differences: * **Positionnal embeddings:** rotary (Su et al., 2021); * **Attention:** multiquery (Shazeer et al., 2019) and FlashAttention (Dao et al., 2022); * **Decoder-block:** parallel attention/MLP with a single layer norm. | **Hyperparameter** | **Value** | **Comment** | |--------------------|-----------|----------------------------------------| | Layers | 32 | | | | 4544 | Increased to compensate for multiquery | | | 64 | Reduced to optimise for FlashAttention | | Vocabulary | 65024 | | | Sequence length | 2048 | | ### Compute Infrastructure #### Hardware Falcon-7B-Instruct was trained on AWS SageMaker, on 32 A100 40GB GPUs in P4d instances. #### Software Falcon-7B-Instruct was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.) ## Citation *Paper coming soon* ðŸ˜Š. In the meanwhile, you can use the following information to cite: To learn more about the pretraining dataset, see the ðŸ““ RefinedWeb paper. ## License Falcon-7B-Instruct is made available under the Apache 2.0 license. ## Contact falconllm@tii.ae",
    "model_explanation_gemini": "Falcon-7B-Instruct is a 7B-parameter English-language chat/instruct model optimized for inference, finetuned from Falcon-7B on mixed datasets to provide conversational responses."
}