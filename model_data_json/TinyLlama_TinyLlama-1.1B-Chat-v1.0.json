{
    "model_id": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "downloads": 1639437,
    "tags": [
        "transformers",
        "safetensors",
        "llama",
        "text-generation",
        "conversational",
        "en",
        "dataset:cerebras/SlimPajama-627B",
        "dataset:bigcode/starcoderdata",
        "dataset:HuggingFaceH4/ultrachat_200k",
        "dataset:HuggingFaceH4/ultrafeedback_binarized",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: apache-2.0 datasets: - cerebras/SlimPajama-627B - bigcode/starcoderdata - HuggingFaceH4/ultrachat_200k - HuggingFaceH4/ultrafeedback_binarized language: - en widget: - example_title: Fibonacci (Python) messages: - role: system content: You are a chatbot who can help code! - role: user content: Write me a function to calculate the first 10 digits of the fibonacci sequence in Python and print it out to the CLI. --- <div align=\"center\"> # TinyLlama-1.1B </div> The TinyLlama project aims to **pretrain** a **1.1B Llama model on 3 trillion tokens**. With some proper optimization, we can achieve this within a span of \"just\" 90 days using 16 A100-40G GPUs ðŸš€ðŸš€. The training has started on 2023-09-01. We adopted exactly the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint. #### This Model This is the chat model finetuned on top of TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T. **We follow HF's Zephyr's training recipe.** The model was \" initially fine-tuned on a variant of the []( dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with ðŸ¤— TRL's on the openbmb/UltraFeedback dataset, which contain 64k prompts and model completions that are ranked by GPT-4.\" #### How to use You will need the transformers>=4.34 Do check the TinyLlama github page for more information.",
    "model_explanation_gemini": "TinyLlama-1.1B-Chat-v1.0 is a compact 1.1B-parameter chatbot model fine-tuned for coding assistance and diverse dialogue generation, optimized for low-resource environments."
}