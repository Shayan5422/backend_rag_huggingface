{
    "model_id": "joeddav/xlm-roberta-large-xnli",
    "downloads": 133451,
    "tags": [
        "transformers",
        "pytorch",
        "tf",
        "safetensors",
        "xlm-roberta",
        "text-classification",
        "tensorflow",
        "zero-shot-classification",
        "multilingual",
        "en",
        "fr",
        "es",
        "de",
        "el",
        "bg",
        "ru",
        "tr",
        "ar",
        "vi",
        "th",
        "zh",
        "hi",
        "sw",
        "ur",
        "dataset:multi_nli",
        "dataset:xnli",
        "arxiv:1911.02116",
        "license:mit",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- language: - multilingual - en - fr - es - de - el - bg - ru - tr - ar - vi - th - zh - hi - sw - ur tags: - text-classification - pytorch - tensorflow datasets: - multi_nli - xnli license: mit pipeline_tag: zero-shot-classification widget: - text: \"За кого вы голосуете в 2020 году?\" candidate_labels: \"politique étrangère, Europe, élections, affaires, politique\" multi_class: true - text: \"لمن تصوت في 2020؟\" candidate_labels: \"السياسة الخارجية, أوروبا, الانتخابات, الأعمال, السياسة\" multi_class: true - text: \"2020'de kime oy vereceksiniz?\" candidate_labels: \"dış politika, Avrupa, seçimler, ticaret, siyaset\" multi_class: true --- # xlm-roberta-large-xnli ## Model Description This model takes xlm-roberta-large and fine-tunes it on a combination of NLI data in 15 languages. It is intended to be used for zero-shot text classification, such as with the Hugging Face ZeroShotClassificationPipeline. ## Intended Usage This model is intended to be used for zero-shot text classification, especially in languages other than English. It is fine-tuned on XNLI, which is a multilingual NLI dataset. The model can therefore be used with any of the languages in the XNLI corpus: - English - French - Spanish - German - Greek - Bulgarian - Russian - Turkish - Arabic - Vietnamese - Thai - Chinese - Hindi - Swahili - Urdu Since the base model was pre-trained trained on 100 different languages, the model has shown some effectiveness in languages beyond those listed above as well. See the full list of pre-trained languages in appendix A of the XLM Roberata paper For English-only classification, it is recommended to use bart-large-mnli or a distilled bart MNLI model. #### With the zero-shot classification pipeline The model can be loaded with the pipeline like so: You can then classify in any of the above languages. You can even pass the labels in one language and the sequence to classify in another: The default hypothesis template is the English, . If you are working strictly within one language, it may be worthwhile to translate this to the language you are working with: #### With manual PyTorch ## Training This model was pre-trained on set of 100 languages, as described in the original paper. It was then fine-tuned on the task of NLI on the concatenated MNLI train set and the XNLI validation and test sets. Finally, it was trained for one additional epoch on only XNLI data where the translations for the premise and hypothesis are shuffled such that the premise and hypothesis for each example come from the same original English example but the premise and hypothesis are of different languages.",
    "model_explanation_gemini": "Performs multilingual zero-shot text classification across 15 languages by leveraging fine-tuned XLM-RoBERTa on NLI data."
}