{
    "model_id": "llava-hf/llava-onevision-qwen2-7b-ov-hf",
    "downloads": 2273949,
    "tags": [
        "transformers",
        "safetensors",
        "llava_onevision",
        "image-text-to-text",
        "vision",
        "conversational",
        "en",
        "zh",
        "dataset:lmms-lab/LLaVA-OneVision-Data",
        "arxiv:2408.03326",
        "license:apache-2.0",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- language: - en - zh license: apache-2.0 tags: - vision - image-text-to-text datasets: - lmms-lab/LLaVA-OneVision-Data library_name: transformers pipeline_tag: image-text-to-text arxiv: 2408.03326 --- # LLaVA-Onevision Model Card !image/png Check out also the Google Colab demo to run Llava on a free-tier Google Colab instance: ![Open In Colab]( Below is the model card of 7B LLaVA-Onevision model which is copied from the original LLaVA-Onevision model card that you can find here. ## Model details **Model type:** LLaVA-Onevision is an open-source multimodal LLM trained by fine-tuning Qwen2 on GPT-generated multimodal instruction-following data. LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos. **Model date:** LLaVA-Onevision-7b-si was added in August 2024. **Paper or resources for more information:** - **Architecture:** SO400M + Qwen2 - **Pretraining Stage:** LCS-558K, 1 epoch, projector - **Mid Stage:** A mixture of 4.7M high-quality synthetic data, 1 epoch, full model - **Final-Image Stage:** A mixture of 3.6M single-image data, 1 epoch, full model - **OneVision Stage:** A mixture of 1.6M single-image/multi-image/video data, 1 epoch, full model - **Precision:** bfloat16 ## How to use the model First, make sure to have installed from branch or . The model supports multi-image and multi-prompt generation. Meaning that you can pass multiple images in your prompt. Make sure also to follow the correct prompt template by applyong chat template: ### Using : Below we used []( checkpoint. ### Using pure : Below is an example script to run generation in precision on a GPU device: ### Model optimization #### 4-bit quantization through library First make sure to install , and make sure to have access to a CUDA compatible GPU device. Simply change the snippet above with: #### Use Flash-Attention 2 to further speed-up generation First make sure to install . Refer to the original repository of Flash Attention regarding that package installation. Simply change the snippet above with: # Citation",
    "model_explanation_gemini": "An open-source multimodal LLM that processes and generates text from single/multiple images and videos, demonstrating strong transfer learning across visual scenarios."
}