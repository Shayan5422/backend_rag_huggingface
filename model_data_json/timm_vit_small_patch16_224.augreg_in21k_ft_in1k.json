{
    "model_id": "timm/vit_small_patch16_224.augreg_in21k_ft_in1k",
    "downloads": 626007,
    "tags": [
        "timm",
        "pytorch",
        "safetensors",
        "image-classification",
        "transformers",
        "dataset:imagenet-1k",
        "dataset:imagenet-21k",
        "arxiv:2106.10270",
        "arxiv:2010.11929",
        "license:apache-2.0",
        "region:us"
    ],
    "description": "--- tags: - image-classification - timm - transformers library_name: timm license: apache-2.0 datasets: - imagenet-1k - imagenet-21k --- # Model card for vit_small_patch16_224.augreg_in21k_ft_in1k A Vision Transformer (ViT) image classification model. Trained on ImageNet-21k and fine-tuned on ImageNet-1k (with additional augmentation and regularization) in JAX by paper authors, ported to PyTorch by Ross Wightman. ## Model Details - **Model Type:** Image classification / feature backbone - **Model Stats:** - Params (M): 22.1 - GMACs: 4.3 - Activations (M): 8.2 - Image size: 224 x 224 - **Papers:** - How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers: - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale: - **Dataset:** ImageNet-1k - **Pretrain Dataset:** ImageNet-21k - **Original:** ## Model Usage ### Image Classification ### Image Embeddings ## Model Comparison Explore the dataset and runtime metrics of this model in timm model results. ## Citation",
    "model_explanation_gemini": "A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k with augmentation and regularization."
}