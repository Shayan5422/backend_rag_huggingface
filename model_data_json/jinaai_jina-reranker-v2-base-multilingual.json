{
    "model_id": "jinaai/jina-reranker-v2-base-multilingual",
    "downloads": 702180,
    "tags": [
        "transformers",
        "pytorch",
        "onnx",
        "safetensors",
        "text-classification",
        "reranker",
        "cross-encoder",
        "transformers.js",
        "custom_code",
        "multilingual",
        "license:cc-by-nc-4.0",
        "autotrain_compatible",
        "region:eu"
    ],
    "description": "--- pipeline_tag: text-classification tags: - transformers - reranker - cross-encoder - transformers.js language: - multilingual inference: false license: cc-by-nc-4.0 library_name: transformers --- <br><br> <p align=\"center\"> <img src=\" alt=\"Jina AI: Your Search Foundation, Supercharged!\" width=\"150px\"> </p> <p align=\"center\"> <b>Trained by <a href=\" AI</b></a>.</b> </p> # jina-reranker-v2-base-multilingual ## Intended Usage & Model Info The **Jina Reranker v2** () is a transformer-based model that has been fine-tuned for text reranking task, which is a crucial component in many information retrieval systems. It is a cross-encoder model that takes a query and a document pair as input and outputs a score indicating the relevance of the document to the query. The model is trained on a large dataset of query-document pairs and is capable of reranking documents in multiple languages with high accuracy. Compared with the state-of-the-art reranker models, including the previous released , the **Jina Reranker v2** model has demonstrated competitiveness across a series of benchmarks targeting for text retrieval, multilingual capability, function-calling-aware and text-to-SQL-aware reranking, and code retrieval tasks. The model is capable of handling long texts with a context length of up to tokens, enabling the processing of extensive inputs. To enable the model to handle long texts that exceed 1024 tokens, the model uses a sliding window approach to chunk the input text into smaller pieces and rerank each chunk separately. The model is also equipped with a flash attention mechanism, which significantly improves the model's performance. # Usage _This model repository is licenced for research and evaluation purposes under CC-BY-NC-4.0. For commercial usage, please refer to Jina AI's APIs, AWS Sagemaker or Azure Marketplace offerings. Please contact us for any further clarifications._ 1. The easiest way to use is to call Jina AI's Reranker API. 2. You can also use the library to interact with the model programmatically. Before you start, install the and libraries: And then: The scores will be a list of floats, where each float represents the relevance score of the corresponding document to the query. Higher scores indicate higher relevance. For instance the returning scores in this case will be: The model gives high relevance scores to the documents that are most relevant to the query regardless of the language of the document. Note that by default, the model uses flash attention, which requires certain types of GPU hardware to run. If you encounter any issues, you can try call with . This will use the standard attention mechanism instead of flash attention. If you want to use flash attention for fast inference, you need to install the following packages: Enjoy the 3x-6x speedup with flash attention! ⚡️⚡️⚡️ 3. You can also use the library to run the model directly in JavaScript (in-browser, Node.js, Deno, etc.)! If you haven't already, you can install the Transformers.js JavaScript library (v3) using: Then, you can use the following code to interact with the model: That's it! You can now use the model in your projects. In addition to the function, the model also provides a function that can be used to rerank documents based on a query. You can use it as follows: Inside the object, you will find the reranked documents along with their scores. You can use this information to further process the documents as needed. The function will automatically chunk the input documents into smaller pieces if they exceed the model's maximum input length. This allows you to rerank long documents without running into memory issues. Specifically, the function will split the documents into chunks of size and rerank each chunk separately. The scores from all the chunks are then combined to produce the final reranking results. You can control the query length and document length in each chunk by setting the and parameters. The function also supports the parameter (default is ) which determines how much overlap there is between adjacent chunks. This can be useful when reranking long documents to ensure that the model has enough context to make accurate predictions. 3. Alternatively, has been integrated with from the library. Before you start, install the libraries: The []( class supports a []( method to get query-document relevance scores, and a []( method to rank all documents given your query. # Evaluation We evaluated Jina Reranker v2 on multiple benchmarks to ensure top-tier performance and search relevance. | Model Name | Model Size | MKQA(nDCG@10, 26 langs) | BEIR(nDCG@10, 17 datasets) | MLDR(recall@10, 13 langs) | CodeSearchNet (MRR@10, 3 tasks) | AirBench (nDCG@10, zh/en) | ToolBench (recall@3, 3 tasks) | TableSearch (recall@3) | | :-----------------------------: | :----------: | ------------------------- | ---------------------------- | --------------------------- | --------------------------------- | --------------------------- | ------------------------------- | ------------------------ | | jina-reranker-v2-multilingual | 278M | 54.83 | 53.17 | 68.95 | 71.36 | 61.33 | 77.75 | 93.31 | | bge-reranker-v2-m3 | 568M | 54.17 | 53.65 | 59.73 | 62.86 | 61.28 | 78.46 | 74.86 | | mmarco-mMiniLMv2-L12-H384-v1 | 118M | 53.37 | 45.40 | 28.91 | 51.78 | 56.46 | 58.39 | 53.60 | | jina-reranker-v1-base-en | 137M | - | 52.45 | - | - | - | 74.13 | 72.89 | Note: - NDCG@10 and MRR@10 measure ranking quality, with higher scores indicating better search results - recall@3 measures the proportion of relevant documents retrieved, with higher scores indicating better search results",
    "model_explanation_gemini": "Reranks documents based on their relevance to a query across multiple languages using a cross-encoder transformer model with multilingual capability and long-text handling."
}