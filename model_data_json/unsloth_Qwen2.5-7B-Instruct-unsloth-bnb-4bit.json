{
    "model_id": "unsloth/Qwen2.5-7B-Instruct-unsloth-bnb-4bit",
    "downloads": 318206,
    "tags": [
        "transformers",
        "safetensors",
        "qwen2",
        "text-generation",
        "unsloth",
        "qwen",
        "conversational",
        "zho",
        "eng",
        "fra",
        "spa",
        "por",
        "deu",
        "ita",
        "rus",
        "jpn",
        "kor",
        "vie",
        "tha",
        "ara",
        "arxiv:2407.10671",
        "base_model:Qwen/Qwen2.5-7B-Instruct",
        "base_model:quantized:Qwen/Qwen2.5-7B-Instruct",
        "license:apache-2.0",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "4-bit",
        "bitsandbytes",
        "region:us"
    ],
    "description": "--- base_model: Qwen/Qwen2.5-7B-Instruct language: - zho - eng - fra - spa - por - deu - ita - rus - jpn - kor - vie - tha - ara library_name: transformers license: apache-2.0 tags: - unsloth - transformers - qwen --- <div> <p style=\"margin-bottom: 0;\"> <strong>See <a href=\" collection</a> for versions of Qwen2.5 including 4-bit formats.</strong> </p> <p style=\"margin-bottom: 0;\"> <em>Unsloth's <a href=\" 4-bit Quants</a> is selectively quantized, greatly improving accuracy over standard 4-bit.</em> </p> <div style=\"display: flex; gap: 5px; align-items: center; \"> <a href=\" <img src=\" width=\"133\"> </a> <a href=\" <img src=\" width=\"173\"> </a> <a href=\" <img src=\" width=\"143\"> </a> </div> <h1 style=\"margin-top: 0rem;\">Finetune LLMs 2-5x faster with 70% less memory via Unsloth!</h2> </div> We have a free Google Colab Tesla T4 notebook for Qwen2.5 (7B) here: ## ‚ú® Finetune for Free All notebooks are **beginner friendly**! Add your dataset, click \"Run All\", and you'll get a 2x faster finetuned model which can be exported to GGUF, vLLM or uploaded to Hugging Face. | Unsloth supports | Free Notebooks | Performance | Memory use | |-----------------|--------------------------------------------------------------------------------------------------------------------------|-------------|----------| | **Llama-3.2 (3B)** | ‚ñ∂Ô∏è Start on Colab-Conversational.ipynb) | 2.4x faster | 58% less | | **Llama-3.2 (11B vision)** | ‚ñ∂Ô∏è Start on Colab-Vision.ipynb) | 2x faster | 60% less | | **Qwen2 VL (7B)** | ‚ñ∂Ô∏è Start on Colab-Vision.ipynb) | 1.8x faster | 60% less | | **Qwen2.5 (7B)** | ‚ñ∂Ô∏è Start on Colab-Alpaca.ipynb) | 2x faster | 60% less | | **Llama-3.1 (8B)** | ‚ñ∂Ô∏è Start on Colab-Alpaca.ipynb) | 2.4x faster | 58% less | | **Phi-3.5 (mini)** | ‚ñ∂Ô∏è Start on Colab | 2x faster | 50% less | | **Gemma 2 (9B)** | ‚ñ∂Ô∏è Start on Colab-Alpaca.ipynb) | 2.4x faster | 58% less | | **Mistral (7B)** | ‚ñ∂Ô∏è Start on Colab-Conversational.ipynb) | 2.2x faster | 62% less | <img src=\" width=\"200\"/> - This Llama 3.2 conversational notebook-Conversational.ipynb) is useful for ShareGPT ChatML / Vicuna templates. - This text completion notebook-Text_Completion.ipynb) is for raw text. This DPO notebook replicates Zephyr. - \\* Kaggle has 2x T4s, but we use 1. Due to overhead, 1x T4 is 5x faster. # Qwen2.5 ## Introduction Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2: - Significantly **more knowledge** and has greatly improved capabilities in **coding** and **mathematics**, thanks to our specialized expert models in these domains. - Significant improvements in **instruction following**, **generating long texts** (over 8K tokens), **understanding structured data** (e.g, tables), and **generating structured outputs** especially JSON. **More resilient to the diversity of system prompts**, enhancing role-play implementation and condition-setting for chatbots. - **Long-context Support** up to 128K tokens and can generate up to 8K tokens. - **Multilingual support** for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. **This repo contains the base 0.5B Qwen2.5 model**, which has the following features: - Type: Causal Language Models - Training Stage: Pretraining - Architecture: transformers with RoPE, SwiGLU, RMSNorm, Attention QKV bias and tied word embeddings - Number of Parameters: 0.49B - Number of Paramaters (Non-Embedding): 0.36B - Number of Layers: 24 - Number of Attention Heads (GQA): 14 for Q and 2 for KV - Context Length: Full 32,768 tokens **We do not recommend using base language models for conversations.** Instead, you can apply post-training, e.g., SFT, RLHF, continued pretraining, etc., on this model. For more details, please refer to our blog, GitHub, and Documentation. ## Requirements The code of Qwen2.5 has been in the latest Hugging face and we advise you to use the latest version of . With , you will encounter the following error: ## Evaluation & Performance Detailed evaluation results are reported in this üìë blog. For requirements on GPU memory and the respective throughput, see results here. ## Citation If you find our work helpful, feel free to give us a cite.",
    "model_explanation_gemini": "A 4-bit quantized version of the Qwen2.5-7B-Instruct model optimized for faster fine-tuning and reduced memory usage while maintaining improved accuracy in multilingual instruction-following tasks."
}