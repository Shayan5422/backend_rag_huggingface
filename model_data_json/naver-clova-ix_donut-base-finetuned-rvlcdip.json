{
    "model_id": "naver-clova-ix/donut-base-finetuned-rvlcdip",
    "downloads": 159190,
    "tags": [
        "transformers",
        "pytorch",
        "vision-encoder-decoder",
        "image-text-to-text",
        "donut",
        "image-to-text",
        "vision",
        "arxiv:2111.15664",
        "license:mit",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: mit tags: - donut - image-to-text - vision --- # Donut (base-sized model, fine-tuned on RVL-CDIP) Donut model fine-tuned on RVL-CDIP. It was introduced in the paper OCR-free Document Understanding Transformer by Geewok et al. and first released in this repository. Disclaimer: The team releasing Donut did not write a model card for this model so this model card has been written by the Hugging Face team. ## Model description Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. !model image ## Intended uses & limitations This model is fine-tuned on RVL-CDIP, a document image classification dataset. We refer to the documentation which includes code examples. ### BibTeX entry and citation info",
    "model_explanation_gemini": "Performs document image classification by encoding images with a Swin Transformer and generating text descriptions using a BART decoder, fine-tuned on the RVL-CDIP dataset."
}