{
    "model_id": "Exscientia/IgT5_unpaired",
    "downloads": 36981,
    "tags": [
        "transformers",
        "safetensors",
        "t5",
        "text2text-generation",
        "antibody language model",
        "antibody",
        "protein language model",
        "arxiv:2403.17889",
        "base_model:Rostlab/prot_t5_xl_uniref50",
        "base_model:finetune:Rostlab/prot_t5_xl_uniref50",
        "license:mit",
        "autotrain_compatible",
        "text-generation-inference",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- tags: - antibody language model - antibody - protein language model base_model: Rostlab/prot_t5_xl_uniref50 license: mit --- # IgT5 unpaired model Model pretrained on protein and antibody sequences using a masked language modeling (MLM) objective. It was introduced in the paper Large scale paired antibody language models. The model is finetuned from ProtT5 using unpaired antibody sequences from the Observed Antibody Space. # Use The encoder part of the model and tokeniser can be loaded using the library The tokeniser is used to prepare batch inputs Note that the tokeniser adds a token at the end of each sequence and pads using the token. For example a batch containing sequences , will be tokenised to and . Sequence embeddings are generated by feeding tokens through the model To obtain a sequence representation, the residue tokens can be averaged over like so",
    "model_explanation_gemini": "Pretrained on protein and antibody sequences using masked language modeling, finetuned from ProtT5 on unpaired antibody data for generating sequence embeddings."
}