{
    "model_id": "nvidia/segformer-b0-finetuned-ade-512-512",
    "downloads": 213515,
    "tags": [
        "transformers",
        "pytorch",
        "tf",
        "safetensors",
        "segformer",
        "vision",
        "image-segmentation",
        "dataset:scene_parse_150",
        "arxiv:2105.15203",
        "license:other",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- license: other tags: - vision - image-segmentation datasets: - scene_parse_150 widget: - src: example_title: House - src: example_title: Castle --- # SegFormer (b0-sized) model fine-tuned on ADE20k SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. Disclaimer: The team releasing SegFormer did not write a model card for this model so this model card has been written by the Hugging Face team. ## Model description SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes. The hierarchical Transformer is first pre-trained on ImageNet-1k, after which a decode head is added and fine-tuned altogether on a downstream dataset. ## Intended uses & limitations You can use the raw model for semantic segmentation. See the model hub to look for fine-tuned versions on a task that interests you. ### How to use Here is how to use this model to classify an image of the COCO 2017 dataset into one of the 1,000 ImageNet classes: For more code examples, we refer to the documentation. ### License The license for this model can be found here. ### BibTeX entry and citation info",
    "model_explanation_gemini": "Performs semantic segmentation on images, fine-tuned for scene parsing using the ADE20k dataset at 512x512 resolution."
}