{
    "model_id": "nm-testing/pixtral-12b-FP8-dynamic",
    "downloads": 130648,
    "tags": [
        "transformers",
        "safetensors",
        "llava",
        "image-text-to-text",
        "vllm",
        "vision",
        "fp8",
        "conversational",
        "en",
        "base_model:mgoin/pixtral-12b",
        "base_model:quantized:mgoin/pixtral-12b",
        "license:apache-2.0",
        "endpoints_compatible",
        "compressed-tensors",
        "region:us"
    ],
    "description": "--- tags: - vllm - vision - fp8 license: apache-2.0 license_link: >- language: - en base_model: mgoin/pixtral-12b library_name: transformers --- # pixtral-12b-FP8-Dynamic ## Model Overview - **Model Architecture:** mgoin/pixtral-12b - **Input:** Vision-Text - **Output:** Text - **Model Optimizations:** - **Weight quantization:** FP8 - **Activation quantization:** FP8 - **Release Date:** 2/24/2025 - **Version:** 1.0 - **Model Developers:** Neural Magic Quantized version of mgoin/pixtral-12b. ### Model Optimizations This model was obtained by quantizing the weights of mgoin/pixtral-12b to FP8 data type, ready for inference with vLLM >= 0.5.2. ## Deployment ### Use with vLLM This model can be deployed efficiently using the vLLM backend, as shown in the example below. vLLM also supports OpenAI-compatible serving. See the documentation for more details. ## Creation This model was created with llm-compressor by running the code snippet below as part a multimodal announcement blog. <details> <summary>Model Creation Code</summary> </details> ## Evaluation The model was evaluated using mistral-evals for vision-related tasks and using lm_evaluation_harness for select text-based benchmarks. The evaluations were conducted using the following commands: <details> <summary>Evaluation Commands</summary> ### Vision Tasks - vqav2 - docvqa - mathvista - mmmu - chartqa ### Text-based Tasks #### MMLU #### HumanEval ##### Generation ##### Sanitization ##### Evaluation </details> ## Accuracy <table border=\"1\"> <thead> <tr> <th>Category</th> <th>Metric</th> <th>mgoin/pixtral-12b</th> <th>neuralmagic/pixtral-12b-FP8-Dynamic</th> <th>Recovery (%)</th> </tr> </thead> <tbody> <tr> <td rowspan=\"6\"><b>Vision</b></td> <td>MMMU (val, CoT)<br><i>explicit_prompt_relaxed_correctness</i></td> <td>48.00</td> <td>50.11</td> <td>104.40%</td> </tr> <tr> <td>VQAv2 (val)<br><i>vqa_match</i></td> <td>78.71</td> <td>78.44</td> <td>99.66%</td> </tr> <tr> <td>DocVQA (val)<br><i>anls</i></td> <td>89.47</td> <td>89.20</td> <td>99.70%</td> </tr> <tr> <td>ChartQA (test, CoT)<br><i>anywhere_in_answer_relaxed_correctness</i></td> <td>81.68</td> <td>81.76</td> <td>100.10%</td> </tr> <tr> <td>Mathvista (testmini, CoT)<br><i>explicit_prompt_relaxed_correctness</i></td> <td>56.50</td> <td>58.70</td> <td>103.89%</td> </tr> <tr> <td><b>Average Score</b></td> <td><b>70.07</b></td> <td><b>71.24</b></td> <td><b>101.67%</b></td> </tr> <tr> <td rowspan=\"2\"><b>Text</b></td> <td>HumanEval <br><i>pass@1</i></td> <td>68.40</td> <td>69.50</td> <td>101.61%</td> </tr> <tr> <td>MMLU (5-shot)</td> <td>71.40</td> <td>69.50</td> <td>97.34%</td> </tr> </tbody> </table> ## Inference Performance This model achieves up to 1.80x speedup in single-stream deployment and up to 1.36x speedup in multi-stream asynchronous deployment, depending on hardware and use-case scenario. The following performance benchmarks were conducted with vLLM version 0.7.2, and GuideLLM. <details> <summary>Benchmarking Command</summary> </details> ### Single-stream performance (measured with vLLM version 0.7.2) <table border=\"1\" class=\"dataframe\"> <thead> <tr> <th></th> <th></th> <th></th> <th style=\"text-align: center;\" colspan=\"2\" >Document Visual Question Answering<br>1680W x 2240H<br>64/128</th> <th style=\"text-align: center;\" colspan=\"2\" >Visual Reasoning <br>640W x 480H<br>128/128</th> <th style=\"text-align: center;\" colspan=\"2\" >Image Captioning<br>480W x 360H<br>0/128</th> </tr> <tr> <th>Hardware</th> <th>Model</th> <th>Average Cost Reduction</th> <th>Latency (s)</th> <th>Queries Per Dollar</th> <th>Latency (s)<th> <th>Queries Per Dollar</th> <th>Latency (s)</th> <th>Queries Per Dollar</th> </tr> </thead> <tbody style=\"text-align: center\"> <tr> <th rowspan=\"3\" valign=\"top\">A6000x1</th> <th>mgoin/pixtral-12b</th> <td></td> <td>5.7</td> <td>796</td> <td>4.8</td> <td>929</td> <td>4.7</td> <td>964</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w8a8</th> <td>1.55</td> <td>3.7</td> <td>1220</td> <td>3.1</td> <td>1437</td> <td>3.0</td> <td>1511</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w4a16</th> <td>2.16</td> <td>3.2</td> <td>1417</td> <td>2.1</td> <td>2093</td> <td>1.9</td> <td>2371</td> </tr> <tr> <th rowspan=\"3\" valign=\"top\">A100x1</th> <th>mgoin/pixtral-12b</th> <td></td> <td>3.0</td> <td>676</td> <td>2.4</td> <td>825</td> <td>2.3</td> <td>859</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w8a8</th> <td>1.38</td> <td>2.2</td> <td>904</td> <td>1.7</td> <td>1159</td> <td>1.7</td> <td>1201</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w4a16</th> <td>1.83</td> <td>1.8</td> <td>1096</td> <td>1.3</td> <td>1557</td> <td>1.2</td> <td>1702</td> </tr> <tr> <th rowspan=\"3\" valign=\"top\">H100x1</th> <th>mgoin/pixtral-12b</th> <td></td> <td>1.8</td> <td>595</td> <td>1.5</td> <td>732</td> <td>1.4</td> <td>764</td> </tr> <tr> <th>neuralmagic/pixtral-12b-FP8-Dynamic</th> <td>1.35</td> <td>1.4</td> <td>767</td> <td>1.1</td> <td>1008</td> <td>1.0</td> <td>1056</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w4a16</th> <td>1.37</td> <td>1.4</td> <td>787</td> <td>1.1</td> <td>1018</td> <td>1.0</td> <td>1065</td> </tr> </tbody> </table> **Use case profiles: Image Size (WxH) / prompt tokens / generation tokens **QPD: Queries per dollar, based on on-demand cost at Lambda Labs (observed on 2/18/2025). ### Multi-stream asynchronous performance (measured with vLLM version 0.7.2) <table border=\"1\" class=\"dataframe\"> <thead> <tr> <th></th> <th></th> <th></th> <th style=\"text-align: center;\" colspan=\"2\" >Document Visual Question Answering<br>1680W x 2240H<br>64/128</th> <th style=\"text-align: center;\" colspan=\"2\" >Visual Reasoning <br>640W x 480H<br>128/128</th> <th style=\"text-align: center;\" colspan=\"2\" >Image Captioning<br>480W x 360H<br>0/128</th> </tr> <tr> <th>Hardware</th> <th>Model</th> <th>Average Cost Reduction</th> <th>Maximum throughput (QPS)</th> <th>Queries Per Dollar</th> <th>Maximum throughput (QPS)</th> <th>Queries Per Dollar</th> <th>Maximum throughput (QPS)</th> <th>Queries Per Dollar</th> </tr> </thead> <tbody style=\"text-align: center\"> <tr> <th rowspan=\"3\" valign=\"top\">A6000x1</th> <th>mgoin/pixtral-12b</th> <td></td> <td>0.6</td> <td>2632</td> <td>0.9</td> <td>4108</td> <td>1.1</td> <td>4774</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w8a8</th> <td>1.50</td> <td>0.9</td> <td>3901</td> <td>1.4</td> <td>6160</td> <td>1.6</td> <td>7292</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w4a16</th> <td>1.41</td> <td>0.6</td> <td>2890</td> <td>1.3</td> <td>5758</td> <td>1.8</td> <td>8312</td> </tr> <tr> <th rowspan=\"3\" valign=\"top\">A100x1</th> <th>mgoin/pixtral-12b</th> <td></td> <td>1.1</td> <td>2291</td> <td>1.8</td> <td>3670</td> <td>2.1</td> <td>4284</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w8a8</th> <td>1.38</td> <td>1.5</td> <td>3096</td> <td>2.5</td> <td>5076</td> <td>3.0</td> <td>5965</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w4a16</th> <td>1.40</td> <td>1.4</td> <td>2728</td> <td>2.6</td> <td>5133</td> <td>3.5</td> <td>6943</td> </tr> <tr> <th rowspan=\"3\" valign=\"top\">H100x1</th> <th>BF16</th> <td></td> <td>2.6</td> <td>2877</td> <td>4.0</td> <td>4372</td> <td>4.7</td> <td>5095</td> </tr> <tr> <th>neuralmagic/pixtral-12b-FP8-Dynamic</th> <td>1.33</td> <td>3.4</td> <td>3753</td> <td>5.4</td> <td>5862</td> <td>6.3</td> <td>6917</td> </tr> <tr> <th>neuralmagic/pixtral-12b-quantized.w4a16</th> <td>1.22</td> <td>2.8</td> <td>3115</td> <td>5.0</td> <td>5511</td> <td>6.2</td> <td>6777</td> </tr> </tbody> </table> **Use case profiles: Image Size (WxH) / prompt tokens / generation tokens **QPS: Queries per second. **QPD: Queries per dollar, based on on-demand cost at Lambda Labs (observed on 2/18/2025).",
    "model_explanation_gemini": "A quantized vision-text model optimized for efficient inference with FP8 precision, excelling in visual question answering and multimodal tasks while maintaining high accuracy."
}