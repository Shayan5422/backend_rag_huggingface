{
    "model_id": "universitytehran/PersianMind-v1.0",
    "downloads": 266947,
    "tags": [
        "transformers",
        "pytorch",
        "safetensors",
        "llama",
        "text-generation",
        "text-generation-inference",
        "multilingual",
        "fa",
        "en",
        "arxiv:2401.06466",
        "license:cc-by-nc-sa-4.0",
        "co2_eq_emissions",
        "autotrain_compatible",
        "region:us"
    ],
    "description": "--- license: cc-by-nc-sa-4.0 language: - multilingual - fa - en library_name: transformers tags: - text-generation-inference inference: false metrics: - bleu - comet - accuracy - perplexity - spearmanr pipeline_tag: text-generation co2_eq_emissions: emissions: 232380 source: \"PersianMind: A Cross-Lingual Persian-English Large Language Model. training_type: \"fine-tuning\" hardware_used: \"4 RTX3090 24GB GPUs\" geographical_location: \"Tehran, Iran\" --- <p align=\"center\"> <img src=\"PersianMind.jpg\" alt=\"PersianMind logo\" width=200/> </p> # <span style=\"font-variant:small-caps;\">PersianMind</span> <span style=\"font-variant:small-caps;\">PersianMind</span> is a cross-lingual Persian-English large language model. The model achieves state-of-the-art results on Persian subset of the <span style=\"font-variant:small-caps;\">Belebele</span> benchmark and the ParsiNLU multiple-choice QA task. It also attains performance comparable to GPT-3.5-turbo in a Persian reading comprehension task. ## Model Description - **Developed by:** Pedram Rostami, Ali Salemi, and Mohammad Javad Dousti - **Model type:** Language model - **Languages:** English and Persian - **License:** CC BY-NC-SA 4.0 (non-commercial use only.) ## How to Get Started with the Model Use the code below to get started with the model. Note that you need to install <code><b>sentencepiece</b></code> and <code><b>accelerate</b></code> libraries along with <code><b>PyTorch</b></code> and <code><b>ðŸ¤—Transformers</b></code> to run this code. ### How to Quantize the Model Quantized models can be run on resource-constrained devices. To quantize the model, you should install the <code><b>bitsandbytes</b></code> library. In order to quantize the model in 8-bit (), use the code below. Alternatively, you can quantize the model in 4-bit () with the following code. ### Evaluating Quantized Models | Model | <span style=\"font-variant:small-caps;\">Belebele</span> (Persian) | Faâ†’En Translation<br>(<span style=\"font-variant:small-caps;\">Comet</span>) | Enâ†’Fa Translation<br>(<span style=\"font-variant:small-caps;\">Comet</span>) | Model Size | Tokens/sec | | :----------------------------------------------------------------: | :--------------------------------------------------------------: | :------------------------------------------------------------------------: | :------------------------------------------------------------------------: | :--------: | :--------: | | <span style=\"font-variant:small-caps;\">PersianMind</span> () | 73.9 | 83.61 | 79.44 | 13.7G | 25.35 | | <span style=\"font-variant:small-caps;\">PersianMind</span> () | 73.7 | 82.32 | 78.61 | 7.2G | 11.36 | | <span style=\"font-variant:small-caps;\">PersianMind</span> () | 70.2 | 82.07 | 80.36 | 3.9G | 24.36 | We evaluated quantized models in various tasks against the original model. Specifically, we evaluated all models using the reading comprehension multiple-choice question-answering benchmark of <span style=\"font-variant:small-caps;\">Belebele</span> (Persian subset) and reported the accuracy of each model. Additionally, we evaluated our models for Persian-to-English and English-to-Persian translation tasks. For this, we utilized the Persian-English subset of the <span style=\"font-variant:small-caps;\">Flores</span>-200 dataset and reported our results using the <span style=\"font-variant:small-caps;\">Comet</span> metric. Furthermore, we calculated the average number of generated tokens per second by each model during running the translation tasks. To understand resource efficiency, we measured the memory usage of each model by employing the function. ## License <span style=\"font-variant:small-caps;\">PersianMind</span> is subject to Meta's LLaMa2 Community License. It is further licensed under CC BY-NC-SA 4.0, which allows non-commercial use of the model. Commercial use of this model requires written agreement which must be obtained from the copyright holders who are listed as developers in this page. If you suspect any violations, please reach out to us. ## Citation If you find this model helpful, please ensure to cite the following paper. **BibTeX:**",
    "model_explanation_gemini": "PersianMind-v1.0 is a cross-lingual Persian-English large language model optimized for text generation, translation, and question-answering tasks, achieving state-of-the-art performance on Persian benchmarks."
}