{
    "model_id": "lmms-lab/llava-onevision-qwen2-7b-ov",
    "downloads": 80043,
    "tags": [
        "transformers",
        "safetensors",
        "llava",
        "text-generation",
        "multimodal",
        "conversational",
        "en",
        "zh",
        "dataset:lmms-lab/LLaVA-OneVision-Data",
        "arxiv:2408.03326",
        "license:apache-2.0",
        "model-index",
        "autotrain_compatible",
        "endpoints_compatible",
        "region:us"
    ],
    "description": "--- datasets: - lmms-lab/LLaVA-OneVision-Data language: - en - zh library_name: transformers license: apache-2.0 metrics: - accuracy tags: - multimodal model-index: - name: llava-onevision-qwen-7b-ov results: - task: type: multimodal dataset: name: AI2D type: ai2d metrics: - type: accuracy value: 81.4 name: accuracy verified: true - task: type: multimodal dataset: name: ChartQA type: chartqa metrics: - type: accuracy value: 80.0 name: accuracy verified: true - task: type: multimodal dataset: name: DocVQA type: docvqa metrics: - type: accuracy value: 90.2 name: accuracy verified: true - task: type: multimodal dataset: name: InfoVQA type: infovqa metrics: - type: accuracy value: 70.7 name: accuracy verified: true - task: type: multimodal dataset: name: MathVerse type: mathverse metrics: - type: accuracy value: 26.2 name: accuracy verified: true - task: type: multimodal dataset: name: MathVista type: mathvista metrics: - type: accuracy value: 63.2 name: accuracy verified: true - task: type: multimodal dataset: name: MMBench type: mmbench metrics: - type: accuracy value: 80.8 name: accuracy verified: true - task: type: multimodal dataset: name: MME-Perception type: mme-perception metrics: - type: score value: 1580 name: score verified: true - task: type: multimodal dataset: name: MME-Cognition type: mme-cognition metrics: - type: score value: 418 name: score verified: true - task: type: multimodal dataset: name: MMMU type: mmmu metrics: - type: accuracy value: 48.8 name: accuracy verified: true - task: type: multimodal dataset: name: MMVet type: mmvet metrics: - type: accuracy value: 57.5 name: accuracy verified: true - task: type: multimodal dataset: name: MMStar type: mmstar metrics: - type: accuracy value: 61.7 name: accuracy verified: true - task: type: multimodal dataset: name: Seed-Bench type: seed-bench metrics: - type: accuracy value: 75.4 name: accuracy verified: true - task: type: multimodal dataset: name: Science-QA type: science-qa metrics: - type: accuracy value: 96.0 name: accuracy verified: true - task: type: multimodal dataset: name: ImageDC type: imagedc metrics: - type: accuracy value: 88.9 name: accuracy verified: true - task: type: multimodal dataset: name: MMLBench type: mmlbench metrics: - type: accuracy value: 77.1 name: accuracy verified: true - task: type: multimodal dataset: name: RealWorldQA type: realworldqa metrics: - type: accuracy value: 66.3 name: accuracy verified: true - task: type: multimodal dataset: name: Vibe-Eval type: vibe-eval metrics: - type: accuracy value: 51.7 name: accuracy verified: true - task: type: multimodal dataset: name: LLaVA-W type: llava-w metrics: - type: accuracy value: 90.7 name: accuracy verified: true - task: type: multimodal dataset: name: LLaVA-Wilder type: l-wilder metrics: - type: accuracy value: 67.8 name: accuracy verified: true - task: type: multimodal dataset: name: ActNet-QA type: actnet-qa metrics: - type: accuracy value: 56.6 name: accuracy verified: true - task: type: multimodal dataset: name: EgoSchema type: egoschema metrics: - type: accuracy value: 60.1 name: accuracy verified: true - task: type: multimodal dataset: name: MLVU type: mlvu metrics: - type: accuracy value: 64.7 name: accuracy verified: true - task: type: multimodal dataset: name: MVBench type: mvbench metrics: - type: accuracy value: 56.7 name: accuracy verified: true - task: type: multimodal dataset: name: NextQA type: nextqa metrics: - type: accuracy value: 79.4 name: accuracy verified: true - task: type: multimodal dataset: name: PercepTest type: percepTest metrics: - type: accuracy value: 49.7 name: accuracy verified: true - task: type: multimodal dataset: name: SeedBench type: seedbench metrics: - type: accuracy value: 56.9 name: accuracy verified: true - task: type: multimodal dataset: name: VideoChatGPT type: videochatgpt metrics: - type: score value: 3.49 name: score verified: true - task: type: multimodal dataset: name: VideoDC type: videodc metrics: - type: score value: 3.75 name: score verified: true - task: type: multimodal dataset: name: VideoMME type: videomme metrics: - type: accuracy value: 58.2 name: accuracy verified: true --- # LLaVA-OneVision !banner Play with the model on the LLaVA OneVision Chat. ## Table of Contents 1. Model Summary 2. Use 3. Limitations 4. Training 5. License 6. Citation ## Model Summary The LLaVA-OneVision models are 0.5/7/72B parameter models trained on LLaVA-OneVision, based on Qwen2 language model with a context window of 32K tokens. - **Repository:** LLaVA-VL/LLaVA-NeXT - **Project Website:** llava-onevision.lmms-lab.com - **Paper:** LLaVA-OneVision - **Point of Contact:** Bo Li - **Languages:** English, Chinese ## Use ### Intended use The model was trained on LLaVA-OneVision Dataset and have the ability to interact with images, multi-image and videos. **Feel free to share your generations in the Community tab!** ### Generation We provide the simple generation process for using our model. For more details, you could refer to Github. # Training ## Model - **Architecture:** SO400M + Qwen2 - **Pretraining Stage:** LCS-558K, 1 epoch, projector - **Mid Stage:** A mixture of 4.7M high-quality synthetic data, 1 epoch, full model - **Final-Image Stage:** A mixture of 3.6M single-image data, 1 epoch, full model - **OneVision Stage:** A mixture of 1.6M single-image/multi-image/video data, 1 epoch, full model - **Precision:** bfloat16 ## Hardware & Software - **GPUs:** 256 * Nvidia Tesla A100 (for whole model series training) - **Orchestration:** Huggingface Trainer - **Neural networks:** PyTorch # Citation"
}