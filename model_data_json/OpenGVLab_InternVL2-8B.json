{
    "model_id": "OpenGVLab/InternVL2-8B",
    "downloads": 373755,
    "tags": [
        "transformers",
        "safetensors",
        "internvl_chat",
        "feature-extraction",
        "internvl",
        "custom_code",
        "image-text-to-text",
        "conversational",
        "multilingual",
        "arxiv:2312.14238",
        "arxiv:2404.16821",
        "arxiv:2410.16261",
        "arxiv:2412.05271",
        "base_model:OpenGVLab/InternViT-300M-448px",
        "base_model:merge:OpenGVLab/InternViT-300M-448px",
        "base_model:internlm/internlm2_5-7b-chat",
        "base_model:merge:internlm/internlm2_5-7b-chat",
        "license:mit",
        "region:us"
    ],
    "description": "--- license: mit pipeline_tag: image-text-to-text library_name: transformers base_model: - OpenGVLab/InternViT-300M-448px - internlm/internlm2_5-7b-chat new_version: OpenGVLab/InternVL2_5-8B base_model_relation: merge language: - multilingual tags: - internvl - custom_code --- # InternVL2-8B [\\[üìÇ GitHub\\]]( [\\[üìú InternVL 1.0\\]]( [\\[üìú InternVL 1.5\\]]( [\\[üìú Mini-InternVL\\]]( [\\[üìú InternVL 2.5\\]]( [\\[üÜï Blog\\]]( [\\[üó®Ô∏è Chat Demo\\]]( [\\[ü§ó HF Demo\\]]( [\\[üöÄ Quick Start\\]](#quick-start) [\\[üìñ Documents\\]]( <div align=\"center\"> <img width=\"500\" alt=\"image\" src=\" </div> ## Introduction We are excited to announce the release of InternVL 2.0, the latest addition to the InternVL series of multimodal large language models. InternVL 2.0 features a variety of **instruction-tuned models**, ranging from 1 billion to 108 billion parameters. This repository contains the instruction-tuned InternVL2-8B model. Compared to the state-of-the-art open-source multimodal large language models, InternVL 2.0 surpasses most open-source models. It demonstrates competitive performance on par with proprietary commercial models across various capabilities, including document and chart comprehension, infographics QA, scene text understanding and OCR tasks, scientific and mathematical problem solving, as well as cultural understanding and integrated multimodal capabilities. InternVL 2.0 is trained with an 8k context window and utilizes training data consisting of long texts, multiple images, and videos, significantly improving its ability to handle these types of inputs compared to InternVL 1.5. For more details, please refer to our blog and GitHub. | Model Name | Vision Part | Language Part | HF Link | MS Link | | :------------------: | :---------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------: | :--------------------------------------------------------------: | :--------------------------------------------------------------------: | | InternVL2-1B | InternViT-300M-448px | Qwen2-0.5B-Instruct | ü§ó link | ü§ñ link | | InternVL2-2B | InternViT-300M-448px | internlm2-chat-1_8b | ü§ó link | ü§ñ link | | InternVL2-4B | InternViT-300M-448px | Phi-3-mini-128k-instruct | ü§ó link | ü§ñ link | | InternVL2-8B | InternViT-300M-448px | internlm2_5-7b-chat | ü§ó link | ü§ñ link | | InternVL2-26B | InternViT-6B-448px-V1-5 | internlm2-chat-20b | ü§ó link | ü§ñ link | | InternVL2-40B | InternViT-6B-448px-V1-5 | Nous-Hermes-2-Yi-34B | ü§ó link | ü§ñ link | | InternVL2-Llama3-76B | InternViT-6B-448px-V1-5 | Hermes-2-Theta-Llama-3-70B | ü§ó link | ü§ñ link | ## Model Details InternVL 2.0 is a multimodal large language model series, featuring models of various sizes. For each size, we release instruction-tuned models optimized for multimodal tasks. InternVL2-8B consists of InternViT-300M-448px, an MLP projector, and internlm2_5-7b-chat. ## Performance ### Image Benchmarks | Benchmark | MiniCPM-Llama3-V-2_5 | InternVL-Chat-V1-5 | InternVL2-8B | | :--------------------------: | :------------------: | :----------------: | :----------: | | Model Size | 8.5B | 25.5B | 8.1B | | | | | | | DocVQA<sub>test</sub> | 84.8 | 90.9 | 91.6 | | ChartQA<sub>test</sub> | - | 83.8 | 83.3 | | InfoVQA<sub>test</sub> | - | 72.5 | 74.8 | | TextVQA<sub>val</sub> | 76.6 | 80.6 | 77.4 | | OCRBench | 725 | 724 | 794 | | MME<sub>sum</sub> | 2024.6 | 2187.8 | 2210.3 | | RealWorldQA | 63.5 | 66.0 | 64.4 | | AI2D<sub>test</sub> | 78.4 | 80.7 | 83.8 | | MMMU<sub>val</sub> | 45.8 | 46.8 | 51.8 | | MMBench-EN<sub>test</sub> | 77.2 | 82.2 | 81.7 | | MMBench-CN<sub>test</sub> | 74.2 | 82.0 | 81.2 | | CCBench<sub>dev</sub> | 45.9 | 69.8 | 75.9 | | MMVet<sub>GPT-4-0613</sub> | - | 62.8 | 60.0 | | MMVet<sub>GPT-4-Turbo</sub> | 52.8 | 55.4 | 54.2 | | SEED-Image | 72.3 | 76.0 | 76.2 | | HallBench<sub>avg</sub> | 42.4 | 49.3 | 45.2 | | MathVista<sub>testmini</sub> | 54.3 | 53.5 | 58.3 | | OpenCompass<sub>avg</sub> | 58.8 | 61.7 | 64.1 | - For more details and evaluation reproduction, please refer to our Evaluation Guide. - We simultaneously use InternVL and VLMEvalKit repositories for model evaluation. Specifically, the results reported for DocVQA, ChartQA, InfoVQA, TextVQA, MME, AI2D, MMBench, CCBench, MMVet (GPT-4-0613), and SEED-Image were tested using the InternVL repository. MMMU, OCRBench, RealWorldQA, HallBench, MMVet (GPT-4-Turbo), and MathVista were evaluated using the VLMEvalKit. ### Video Benchmarks | Benchmark | VideoChat2-HD-Mistral | Video-CCAM-9B | InternVL2-4B | InternVL2-8B | | :-------------------------: | :-------------------: | :-----------: | :----------: | :----------: | | Model Size | 7B | 9B | 4.2B | 8.1B | | | | | | | | MVBench | 60.4 | 60.7 | 63.7 | 66.4 | | MMBench-Video<sub>8f</sub> | - | - | 1.10 | 1.19 | | MMBench-Video<sub>16f</sub> | - | - | 1.18 | 1.28 | | Video-MME<br>w/o subs | 42.3 | 50.6 | 51.4 | 54.0 | | Video-MME<br>w subs | 54.6 | 54.9 | 53.4 | 56.9 | - We evaluate our models on MVBench and Video-MME by extracting 16 frames from each video, and each frame was resized to a 448x448 image. ### Grounding Benchmarks | Model | avg. | RefCOCO<br>(val) | RefCOCO<br>(testA) | RefCOCO<br>(testB) | RefCOCO+<br>(val) | RefCOCO+<br>(testA) | RefCOCO+<br>(testB) | RefCOCO‚Äëg<br>(val) | RefCOCO‚Äëg<br>(test) | | :----------------------------: | :--: | :--------------: | :----------------: | :----------------: | :---------------: | :-----------------: | :-----------------: | :----------------: | :-----------------: | | UNINEXT-H<br>(Specialist SOTA) | 88.9 | 92.6 | 94.3 | 91.5 | 85.2 | 89.6 | 79.8 | 88.7 | 89.4 | | | | | | | | | | | | | Mini-InternVL-<br>Chat-2B-V1-5 | 75.8 | 80.7 | 86.7 | 72.9 | 72.5 | 82.3 | 60.8 | 75.6 | 74.9 | | Mini-InternVL-<br>Chat-4B-V1-5 | 84.4 | 88.0 | 91.4 | 83.5 | 81.5 | 87.4 | 73.8 | 84.7 | 84.6 | | InternVL‚ÄëChat‚ÄëV1‚Äë5 | 88.8 | 91.4 | 93.7 | 87.1 | 87.0 | 92.3 | 80.9 | 88.5 | 89.3 | | | | | | | | | | | | | InternVL2‚Äë1B | 79.9 | 83.6 | 88.7 | 79.8 | 76.0 | 83.6 | 67.7 | 80.2 | 79.9 | | InternVL2‚Äë2B | 77.7 | 82.3 | 88.2 | 75.9 | 73.5 | 82.8 | 63.3 | 77.6 | 78.3 | | InternVL2‚Äë4B | 84.4 | 88.5 | 91.2 | 83.9 | 81.2 | 87.2 | 73.8 | 84.6 | 84.6 | | InternVL2‚Äë8B | 82.9 | 87.1 | 91.1 | 80.7 | 79.8 | 87.9 | 71.4 | 82.7 | 82.7 | | InternVL2‚Äë26B | 88.5 | 91.2 | 93.3 | 87.4 | 86.8 | 91.0 | 81.2 | 88.5 | 88.6 | | InternVL2‚Äë40B | 90.3 | 93.0 | 94.7 | 89.2 | 88.5 | 92.8 | 83.6 | 90.3 | 90.6 | | InternVL2-<br>Llama3‚Äë76B | 90.0 | 92.2 | 94.8 | 88.4 | 88.8 | 93.1 | 82.8 | 89.5 | 90.3 | - We use the following prompt to evaluate InternVL's grounding ability: Limitations: Although we have made efforts to ensure the safety of the model during the training process and to encourage the model to generate text that complies with ethical and legal requirements, the model may still produce unexpected outputs due to its size and probabilistic generation paradigm. For example, the generated responses may contain biases, discrimination, or other harmful content. Please do not propagate such content. We are not responsible for any consequences resulting from the dissemination of harmful information. ## Quick Start We provide an example code to run using . > Please use transformers>=4.37.2 to ensure the model works normally. ### Model Loading #### 16-bit (bf16 / fp16) #### BNB 8-bit Quantization #### Multiple GPUs The reason for writing the code this way is to avoid errors that occur during multi-GPU inference due to tensors not being on the same device. By ensuring that the first and last layers of the large language model (LLM) are on the same device, we prevent such errors. ### Inference with Transformers #### Streaming Output Besides this method, you can also use the following code to get streamed output. ## Finetune Many repositories now support fine-tuning of the InternVL series models, including InternVL, SWIFT, XTurner, and others. Please refer to their documentation for more details on fine-tuning. ## Deployment ### LMDeploy LMDeploy is a toolkit for compressing, deploying, and serving LLMs & VLMs. LMDeploy abstracts the complex inference process of multi-modal Vision-Language Models (VLM) into an easy-to-use pipeline, similar to the Large Language Model (LLM) inference pipeline. #### A 'Hello, world' Example If occurs while executing this case, please install the required dependency packages as prompted. #### Multi-images Inference When dealing with multiple images, you can put them all in one list. Keep in mind that multiple images will lead to a higher number of input tokens, and as a result, the size of the context window typically needs to be increased. > Warning: Due to the scarcity of multi-image conversation data, the performance on multi-image tasks may be unstable, and it may require multiple attempts to achieve satisfactory results. #### Batch Prompts Inference Conducting inference with batch prompts is quite straightforward; just place them within a list structure: #### Multi-turn Conversation There are two ways to do the multi-turn conversations with the pipeline. One is to construct messages according to the format of OpenAI and use above introduced method, the other is to use the interface. #### Service LMDeploy's enables models to be easily packed into services with a single command. The provided RESTful APIs are compatible with OpenAI's interfaces. Below are an example of service startup: To use the OpenAI-style interface, you need to install OpenAI: Then, use the code below to make the API call: ## License This project is released under the MIT License. This project uses the pre-trained internlm2_5-7b-chat as a component, which is licensed under the Apache License 2.0. ## Citation If you find this project useful in your research, please consider citing:",
    "model_explanation_gemini": "An 8-billion-parameter multimodal instruction-tuned model excelling in image-text tasks like document comprehension, OCR, infographics QA, and cultural understanding with multilingual capabilities."
}