{
    "model_id": "ai4bharat/indictrans2-en-indic-1B",
    "downloads": 108188,
    "tags": [
        "transformers",
        "pytorch",
        "safetensors",
        "IndicTrans",
        "text2text-generation",
        "indictrans2",
        "translation",
        "ai4bharat",
        "multilingual",
        "custom_code",
        "as",
        "bn",
        "brx",
        "doi",
        "en",
        "gom",
        "gu",
        "hi",
        "kn",
        "ks",
        "kas",
        "mai",
        "ml",
        "mr",
        "mni",
        "mnb",
        "ne",
        "or",
        "pa",
        "sa",
        "sat",
        "sd",
        "snd",
        "ta",
        "te",
        "ur",
        "dataset:flores-200",
        "dataset:IN22-Gen",
        "dataset:IN22-Conv",
        "arxiv:2305.16307",
        "license:mit",
        "autotrain_compatible",
        "region:us"
    ],
    "description": "--- language: - as - bn - brx - doi - en - gom - gu - hi - kn - ks - kas - mai - ml - mr - mni - mnb - ne - or - pa - sa - sat - sd - snd - ta - te - ur language_details: >- asm_Beng, ben_Beng, brx_Deva, doi_Deva, eng_Latn, gom_Deva, guj_Gujr, hin_Deva, kan_Knda, kas_Arab, kas_Deva, mai_Deva, mal_Mlym, mar_Deva, mni_Beng, mni_Mtei, npi_Deva, ory_Orya, pan_Guru, san_Deva, sat_Olck, snd_Arab, snd_Deva, tam_Taml, tel_Telu, urd_Arab tags: - indictrans2 - translation - ai4bharat - multilingual license: mit datasets: - flores-200 - IN22-Gen - IN22-Conv metrics: - bleu - chrf - chrf++ - comet inference: false --- # IndicTrans2 This is the model card of IndicTrans2 En-Indic 1.1B variant. Here are the metrics for the particular checkpoint. Please refer to of the preprint for further details on model training, intended use, data, metrics, limitations and recommendations. ### Usage Instructions Please refer to the github repository for a detail description on how to use HF compatible IndicTrans2 models for inference. ### ðŸ“¢ Long Context IT2 Models - New RoPE based IndicTrans2 models which are capable of handling sequence lengths **upto 2048 tokens** are available here - These models can be used by just changing the parameter. Please read the model card of the RoPE-IT2 models for more information about the generation. - It is recommended to run these models with for efficient generation. ### Citation If you consider using our work then please cite using:",
    "model_explanation_gemini": "Translates text between English and multiple Indian languages, handling up to 2048 tokens per sequence."
}